[
  {
    "objectID": "recscal_dcca.html",
    "href": "recscal_dcca.html",
    "title": "Reciprocal scaling with dcCA",
    "section": "",
    "text": "Code\n# Paths\nlibrary(here)\n\n# Multivariate analysis\nlibrary(ade4)\nlibrary(adegraphics)\n\n# dc-CA\nsource(here(\"functions/dpcaiv2-ade4.R\"))\n\n# Reciprocal scaling\nsource(here(\"functions/reciprocal.R\"))\n\n# Matrix algebra\nlibrary(expm)\n\n# Plots\nsource(here(\"functions/plot.R\"))\nlibrary(gridExtra)"
  },
  {
    "objectID": "recscal_dcca.html#introduction",
    "href": "recscal_dcca.html#introduction",
    "title": "Reciprocal scaling with dcCA",
    "section": "Introduction",
    "text": "Introduction\nThis is an extension of reciprocal scaling defined for correspondence analysis by Thioulouse and Chessel (1992) to double-constrained correspondence analysis.\nHere, we have 3 matrices:\n\nA data matrix \\(Y\\) (\\(r \\times c\\))\nA matrix of predictor variables \\(E\\) (\\(r \\times l\\))\nA matrix of predictor variables \\(T\\) (\\(c \\times k\\))\n\n\n\n\n\n\n\n(r &lt;- dim(Y)[1])\n\n[1] 26\n\n(c &lt;- dim(Y)[2])\n\n[1] 21\n\n(l &lt;- ncol(E))\n\n[1] 6\n\n(k &lt;- ncol(T_))\n\n[1] 7"
  },
  {
    "objectID": "recscal_dcca.html#computation",
    "href": "recscal_dcca.html#computation",
    "title": "Reciprocal scaling with dcCA",
    "section": "Computation",
    "text": "Computation\n\nFrom dcCA scores\nWe compute the \\(H^{dcCA}_k(i, j)\\) from the LC scores computed with CCA (noted \\(L^{dcCA}\\) and \\(C^{dcCA}\\)). This formula is a direct extension of formula (11) in Thioulouse and Chessel (1992) but we replace the ordination scores obtained with CA with the ordination scores obtained with dcCA.\n\\[\nH_k^{dcCA}(i, j) = \\frac{L^{dcCA}_k(i) + C^{dcCA}_k(j)}{\\sqrt{2 \\lambda_k \\mu_k}}\n\\]\n\nYdf &lt;- as.data.frame(Y)\nca &lt;- dudi.coa(Ydf, \n               scannf = FALSE,\n               nf = min(r - 1, c - 1))\n\nneig &lt;- min(c(k, l))\ndcca &lt;- dpcaiv2(dudi = ca, \n                dfR = E,\n                dfQ = T_,\n                scannf = FALSE, \n                nf = neig)\n\nL_dcca &lt;- dcca$li\nC_dcca &lt;- dcca$co\n\nlambda_dcca &lt;- dcca$eig\nmu_dcca &lt;- 1 + sqrt(lambda_dcca)\n\nWe also compute reciprocal scaling for comparison:\n\nrec_dcca &lt;- reciprocal.dpcaiv(dcca)\n\n\n# Transform matrix to count table\nYfreq &lt;- as.data.frame(as.table(Y))\ncolnames(Yfreq) &lt;- c(\"row\", \"col\", \"Freq\")\n\n# Remove the cells with no observation\nYfreq0 &lt;- Yfreq[-which(Yfreq$Freq == 0),]\nYfreq0$colind &lt;- match(Yfreq0$col, colnames(Y)) # match index and species names\n\n\n# Initialize results matrix\nH_dcca &lt;- matrix(nrow = nrow(Yfreq0), \n                 ncol = length(lambda_dcca))\n\nfor (kl in 1:length(lambda_dcca)) { # For each axis\n  ind &lt;- 1 # initialize row index\n  for (obs in 1:nrow(Yfreq0)) { # For each observation\n    i &lt;- Yfreq0$row[obs]\n    j &lt;- Yfreq0$col[obs]\n    H_dcca[ind, kl] &lt;- (L_dcca[i, kl] + C_dcca[j, kl])/sqrt(2*lambda_dcca[kl]*mu_dcca[kl])\n    ind &lt;- ind + 1\n  }\n}\n\n\n\nFrom canonical correlation analysis\nTo perform the cancor, we compute the inflated tables \\(R\\) (\\(\\omega \\times l\\)) and \\(C\\) (\\(\\omega \\times k\\)) from \\(E\\) (\\(r \\times l\\)) and \\(T\\) (\\(r \\times k\\)). \\(R\\) and \\(C\\) are respectively equivalents to \\(E\\) and \\(T\\) where rows of are duplicated as many times as there are correspondences in \\(Y\\).\nWe take the frequency table defined before and use it to compute the inflated tables (with weights):\n\n# Create indicator tables\ntabR &lt;- acm.disjonctif(as.data.frame(Yfreq0$row))\ntabR &lt;- as.matrix(tabR) %*% as.matrix(E)\ntabC &lt;- acm.disjonctif(as.data.frame(Yfreq0$col))\ntabC &lt;- as.matrix(tabC) %*% as.matrix(T_)\n\n# Get weights\nwt &lt;- Yfreq0$Freq\n\nBelow are the first lines of tables \\(R\\) and \\(C\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocation\nelevation\npatch_area\nperc_forests\nperc_grasslands\nShannonLandscapeDiv\n\n\n\n\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n0\n30\n7.92\n16.4129\n43.4066\n0.274\n\n\n0\n430\n83.24\n24.4526\n28.4995\n0.274\n\n\n0\n420\n140.83\n41.9966\n34.2412\n0.260\n\n\n0\n400\n140.83\n41.9966\n34.2412\n0.260\n\n\n0\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n0\n470\n7.84\n19.3087\n56.4031\n0.253\n\n\n0\n110\n21.15\n23.4668\n49.8908\n0.262\n\n\n0\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n0\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n0\n40\n6.66\n7.4729\n66.0061\n0.240\n\n\n0\n160\n5.63\n32.3516\n45.0477\n0.253\n\n\n0\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n1\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1\n30\n7.92\n16.4129\n43.4066\n0.274\n\n\n1\n430\n83.24\n24.4526\n28.4995\n0.274\n\n\n1\n420\n140.83\n41.9966\n34.2412\n0.260\n\n\n1\n400\n140.83\n41.9966\n34.2412\n0.260\n\n\n1\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n1\n470\n7.84\n19.3087\n56.4031\n0.253\n\n\n1\n110\n21.15\n23.4668\n49.8908\n0.262\n\n\n1\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n1\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n1\n40\n6.66\n7.4729\n66.0061\n0.240\n\n\n1\n160\n5.63\n32.3516\n45.0477\n0.253\n\n\n1\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n0\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n1\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n1\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n1\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n\n\n\n\n\n\nbiog\nforag\nmass\ndiet\nmove\nnest\neggs\n\n\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n2\n2\n3\n1\n2\n2\n\n\n1\n1\n1\n2\n1\n2\n2\n\n\n1\n1\n1\n2\n1\n2\n2\n\n\n1\n1\n1\n2\n1\n2\n2\n\n\n1\n1\n1\n2\n1\n2\n2\n\n\n\n\n\nThen, we perform a canonical correlation on the scaled tables \\(R_{scaled}\\) and \\(C_{scaled}\\). We find the coefficients \\(\\rho\\) and \\(\\gamma\\) maximizing the correlation between the scores \\(S_R = R_{scaled} \\rho\\) and \\(S_C = C_{scaled} \\gamma\\).\n\n# Center scale tables\ntabR_scaled &lt;- scalewt(tabR, wt)\ntabC_scaled &lt;- scalewt(tabC, wt)\n\nres &lt;- cancor(diag(sqrt(wt)) %*% tabR_scaled, \n              diag(sqrt(wt)) %*% tabC_scaled, \n              xcenter = FALSE, ycenter = FALSE)\n# res gives the coefficients of the linear combinations that maximizes the correlation between the 2 dimensions\ndim(res$xcoef) # l columns -&gt; R_scaled is of full rank\n\n[1] 6 6\n\ndim(res$ycoef) # k columns -&gt; C_scaled is of full rank\n\n[1] 7 7\n\n# Compute these scores from this coef\nscoreR &lt;- tabR_scaled[, 1:l]  %*% res$xcoef\nscoreC &lt;- tabC_scaled[, 1:k]  %*% res$ycoef\n\nWe have \\(H = (S_R + S_C)_{scaled}\\).\n\n# Get H\nscoreRC &lt;- scoreR[, 1:l] + scoreC[, 1:l] # here we have l &lt; k so l axes\nscoreRC_scaled &lt;- scalewt(scoreRC, wt = wt)\n\n\n\nPlot\n\n\nCode\nmultiplot(indiv_row = H_dcca, \n          indiv_row_lab = paste0(\"site \", Yfreq0$row, \"/\", Yfreq0$col),\n          row_color = \"black\", eig = lambda_dcca)"
  },
  {
    "objectID": "recscal.html",
    "href": "recscal.html",
    "title": "Reciprocal scaling",
    "section": "",
    "text": "Code\n# Paths\nlibrary(here)\n\n# Multivariate analysis\nlibrary(ade4)\nlibrary(adegraphics)\n\n# dc-CA\nsource(here(\"functions/dpcaiv2-ade4.R\"))\n\n# Reciprocal scaling\nsource(here(\"functions/reciprocal.R\"))\n\n# Matrix algebra\nlibrary(expm)\n\n# Plots\nsource(here(\"functions/plot.R\"))\nlibrary(gridExtra)"
  },
  {
    "objectID": "recscal.html#introduction",
    "href": "recscal.html#introduction",
    "title": "Reciprocal scaling",
    "section": "Introduction",
    "text": "Introduction\nReciprocal scaling was introduced by Thioulouse and Chessel (1992). It is a technique allowing to compute the coordinates of each row/column cell in the multivariate space, which then allows to get the conditional mean and variance per row or column.\nThis technique was initially defined in the frame of correspondence analysis, but it is extended to canonical CA in reciprocal scaling (CCA) and double-constrained CA in in reciprocal scaling (dcCA).\nHere, we will analyze matrix \\(Y\\):\n\n\n\n\n\n\n(r &lt;- dim(Y)[1])\n\n[1] 26\n\n(c &lt;- dim(Y)[2])\n\n[1] 21"
  },
  {
    "objectID": "recscal.html#computation",
    "href": "recscal.html#computation",
    "title": "Reciprocal scaling",
    "section": "Computation",
    "text": "Computation\n\n\n\n\n\n\nTL;DR\n\n\n\nReciprocal scaling gives a score per correspondence (row/column pair) on each ordination axis (instead of a score per row and per column in CA).\nThese scores can be presented in a matrix \\(H\\) (\\(\\omega \\times K\\)). Here, \\(\\omega = \\sum_Y y_i \\neq 0\\) (number of nonzero cells in \\(Y\\), also called correspondences) and \\(K\\) is the number of eigenvalues (axes) of the CA. Each row of \\(H\\) corresponds to a row/column pair.\nFrom CA scores\n\\(H\\) can be computed from the scores of the correspondence analysis of \\(Y\\). The score \\(H_k(i, j)\\) for row \\(i\\), column \\(j\\) and axis \\(k\\) is:\n\\[\nH_k(i, j) = \\frac{L_k(i) + C_k(j)}{\\sqrt{2 \\lambda_k \\mu_k}}\n\\]\nif \\(y_{ij} = 0\\) then the score \\(H_k(i, j)\\) is not defined.\nFrom canonical correlation analysis\nWe transform the table \\(Y\\) in two tables:\n\n\\(R\\) (\\(\\omega \\times r\\))\n\\(C\\) (\\(\\omega \\times c\\))\n\nEach row of \\(R\\) and \\(C\\) contains an indicator vector. The 1 is in the column corresponding to the row (for \\(R\\)) or column (for \\(C\\)) of the nonzero cell in \\(Y\\).\nAdditionally, we have a weights vector \\(w = y_{ij}\\) (the same for \\(R\\) and \\(C\\)).\nThen, we perform a (weighted) canonical correlation analysis of \\(R_{scaled}\\) and \\(C_{scaled}\\). \\(X_{scaled}\\) corresponds to \\(X\\) center and scaled (here, with the weights \\(w\\)).\nCanonical correlation analysis gives the vectors of canonical coefficients \\(\\rho\\) and \\(\\gamma\\) (resp. for \\(R_{scaled}\\) and \\(C_{scaled}\\)). The scores of the matrices are then \\(S_R = R_{scaled} \\rho\\) and \\(S_C = C_{scaled} \\gamma\\).\nFinally, \\(H\\) is computed as:\n\\[\nH = (S_R + S_C)_{scaled}\n\\]\n\n\nThe coordinates of row (site) \\(i\\) and column (species) \\(j\\) in the multivariate space on axis \\(k\\) \\(H_k(i, j)\\) can be computed in 2 ways:\n\nfrom the coordinates of the CA\nfrom the canonical correlation analysis\n\n\nFrom CA\nWe perform the CA of table \\(Y\\) (\\(r \\times c\\)).\n\nYdf &lt;- as.data.frame(Y)\nca &lt;- dudi.coa(Ydf, \n               scannf = FALSE,\n               nf = min(r - 1, c - 1))\n\nL &lt;- ca$li\nC &lt;- ca$co\n\nlambda &lt;- ca$eig\n\nWe compute the reciprocal scaling score for comparison:\n\nrec_ca &lt;- reciprocal.coa(ca)\n\nWe can get the coordinates of row-column pairs (ie each cell in the original table \\(Y\\)) with:\n\\[\nH_k(i, j) = \\frac{L_k(i) + C_k(j)}{\\sqrt{2 \\lambda_k \\mu_k}}\n\\]\nwhere \\(L_k(i)\\) is the coordinate of row (site) \\(i\\) on axis \\(k\\) and \\(C_k(j)\\) is the same for column (species) \\(j\\) and \\(\\mu_k = 1 + \\sqrt{\\lambda_k}\\).\n\nmu &lt;- 1 + sqrt(lambda)\n\nThe result is a matrix \\(H\\) with \\(\\omega = \\sum_Y y_{ij} \\neq 0\\) rows (one row for each nonzero cell in \\(Y\\), called correspondences) and \\(K\\) columns (one column per principal axis of the CA):\n\n\n\n\n\nLet’s compute \\(H\\) from the CA scores with our example dataset.\n\n# Transform matrix to count table\nYfreq &lt;- as.data.frame(as.table(Y))\ncolnames(Yfreq) &lt;- c(\"row\", \"col\", \"Freq\")\n\n# Remove the cells with no observation\nYfreq0 &lt;- Yfreq[-which(Yfreq$Freq == 0),]\nYfreq0$colind &lt;- match(Yfreq0$col, colnames(Y)) # match index and species names\n\n# Initialize results matrix\nH &lt;- matrix(nrow = nrow(Yfreq0), \n            ncol = length(lambda))\n\nfor (k in 1:length(lambda)) { # For each axis\n  ind &lt;- 1 # initialize row index\n  for (obs in 1:nrow(Yfreq0)) { # For each observation\n    i &lt;- Yfreq0$row[obs]\n    j &lt;- Yfreq0$col[obs]\n    H[ind, k] &lt;- (L[i, k] + C[j, k])/sqrt(2*lambda[k]*mu[k])\n    ind &lt;- ind + 1\n  }\n}\n\n\n\nFrom canonical correlation analysis\nCanonical correlation analysis is the extension of correlation to multidimensional analysis and allows to find the coefficients to maximize the correlation between the columns of two matrices.\nFirst, we compute the inflated tables \\(R\\) (\\(\\omega \\times r\\)) and \\(C\\) (\\(\\omega \\times c\\)) from \\(Y\\) (\\(r \\times c\\)).\nWe take the frequency table defined before and use it to compute the inflated tables (with weights):\n\n# Create indicator tables\ntabR &lt;- acm.disjonctif(as.data.frame(Yfreq0$row))\ncolnames(tabR) &lt;- rownames(Y)\ntabC &lt;- acm.disjonctif(as.data.frame(Yfreq0$col))\ncolnames(tabC) &lt;- colnames(Y)\n\n# Get weights\nwt &lt;- Yfreq0$Freq\n\nBelow are the first lines of tables \\(R\\) and \\(C\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\n\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsp1\nsp2\nsp3\nsp4\nsp5\nsp6\nsp7\nsp8\nsp9\nsp10\nsp11\nsp12\nsp13\nsp14\nsp15\nsp16\nsp17\nsp18\nsp19\nsp21\nsp22\n\n\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nThen, we perform a canonical correlation on the scaled tables \\(R_{scaled}\\) and \\(C_{scaled}\\). We find the coefficients \\(\\rho\\) and \\(\\gamma\\) maximizing the correlation between the scores \\(S_R = R_{scaled} \\rho\\) and \\(S_C = C_{scaled} \\gamma\\).\n\n# Center scale tables\ntabR_scaled &lt;- scalewt(tabR, wt)\ntabC_scaled &lt;- scalewt(tabC, wt)\n\nres &lt;- cancor(diag(sqrt(wt)) %*% tabR_scaled, \n              diag(sqrt(wt)) %*% tabC_scaled, \n              xcenter = FALSE, ycenter = FALSE)\n# res gives the coefficients of the linear combinations that maximizes the correlation between the 2 dimensions\nncol(res$xcoef) # r-1 columns -&gt; R_scaled is not of full rank\n\n[1] 25\n\nncol(res$ycoef) # c-1 columns -&gt; C_scaled is not of full rank\n\n[1] 20\n\n# Compute these scores from this coef\nscoreR &lt;- tabR_scaled[, 1:(r-1)]  %*% res$xcoef\nscoreC &lt;- tabC_scaled[, 1:(c-1)]  %*% res$ycoef\n\nWe have \\(H = (S_R + S_C)_{scaled}\\).\n\n# Get H\nscoreRC &lt;- scoreR[, 1:(c-1)] + scoreC[, 1:(c-1)] # here c-1 &lt; r so c-1 axes\nscoreRC_scaled &lt;- scalewt(scoreRC, wt = wt)\n\n\n\nPlot\nThe scores \\(H\\) are displayed below on the first two axes:\n\n\nCode\nmultiplot(indiv_row = H, \n          indiv_row_lab = paste0(\"site \", Yfreq0$row, \"/\", Yfreq0$col),\n          row_color = \"black\", eig = lambda)"
  },
  {
    "objectID": "recscal.html#conditional-means-and-variances",
    "href": "recscal.html#conditional-means-and-variances",
    "title": "Reciprocal scaling",
    "section": "Conditional means and variances",
    "text": "Conditional means and variances\n\n\n\n\n\n\nTL;DR\n\n\n\nOnce we have the correspondences scores, we can group them by row (site) or column (species) to compute conditional summary statistics:\n\nconditional mean for site \\(i\\) or species \\(j\\) (for each axis \\(k\\))\nconditional variance for site \\(i\\) or species \\(j\\) (for each axis \\(k\\))\nconditional covariance for site \\(i\\) or species \\(j\\) (between axes \\(k\\) and \\(l\\))\n\nThese conditional statistics can be computed using \\(H_k(i,j)\\) or using the CA scores:\nFormulas using \\(H_k(i,j)\\)\n\n\n\n\n\n\n\n\n\nRows (sites)\nColumns (species)\n\n\n\n\nMean for axis \\(k\\)\n\\[m_k(i) = \\frac{1}{y_{i\\cdot}} \\sum_{j = 1}^c y_{ij} H_k(i,j)\\]\n\\[m_k(j) = \\frac{1}{y_{\\cdot j}} \\sum_{i = 1}^r y_{ij} H_k(i,j)\\]\n\n\nVariance for axis \\(k\\)\n\\[s_k^2(i) = \\frac{1}{y_{i\\cdot}} \\sum_{j = 1}^c y_{ij} \\left(H_k(i,j) - m_k(i)\\right)^2\\]\n\\[s_k^2(j) = \\frac{1}{y_{\\cdot j}} \\sum_{i = 1}^r y_{ij} \\left(H_k(i,j) - m_k(j)\\right)^2\\]\n\n\nCovariance between axes \\(k\\), \\(l\\)\n\\[c_{kl}(i) = \\frac{1}{y_{i \\cdot}}\\sum_{j=1}^c y_{ij}H_k(i, j)H_l(i, j) - m_k(i)m_l(i)\\]\n\\[c_{kl}(j) = \\frac{1}{y_{\\cdot j}}\\sum_{i=1}^r y_{ij}H_k(i, j)H_l(i, j) - m_k(j)m_l(j)\\]\n\n\n\nFormulas using CA scores\n\n\n\n\n\n\n\n\n\nRows (sites)\nColumns (species)\n\n\n\n\nMean for axis \\(k\\)\n\\[m_k(i) = \\frac{\\sqrt{\\mu_k}}{\\sqrt{2\\lambda_k}} L_k(i)\\]\n\\[m_k(j) = \\frac{\\sqrt{\\mu_k}}{\\sqrt{2\\lambda_k}} C_k(j)\\]\n\n\nVariance for axis \\(k\\)\n\\[s^2_k(i) = \\frac{1}{2\\lambda_k\\mu_k} \\left( \\frac{1}{y_{i \\cdot}} \\sum_{j = 1}^c \\left(y_{ij} C_k^2(j) \\right) - \\lambda_k L_k^2(i) \\right)\\]\n\\[s^2_k(j) = \\frac{1}{2\\lambda_k\\mu_k} \\left( \\frac{1}{y_{\\cdot j}} \\sum_{i = 1}^r \\left(y_{ij} L_k^2(i) \\right) - \\lambda_k C_k^2(j) \\right)\\]\n\n\nCovariance between axes \\(k\\), \\(l\\)\n\\[c_{kl}(i) = \\frac{1}{2\\sqrt{\\lambda_k \\lambda_l} \\sqrt{\\mu_k \\mu_l}} \\left( \\frac{1}{y_{i \\cdot}} \\sum_{j = 1}^c y_{ij} C_k(j) C_l(j) - \\sqrt{\\lambda_k \\lambda_l} L_k(i)L_l(i) \\right)\\]\n\\[c_{kl}(j) = \\frac{1}{2\\sqrt{\\lambda_k \\lambda_l} \\sqrt{\\mu_k \\mu_l}} \\left( \\frac{1}{y_{\\cdot j}} \\sum_{i = 1}^r y_{ij} L_k(i) L_l(i) - \\sqrt{\\lambda_k \\lambda_l} C_k(j)C_l(j) \\right)\\]\n\n\n\nNote: formulas for the variances \\(s^2_k(i)\\) and \\(s^2_k(j)\\) are different from formula (15) in Thioulouse and Chessel (1992) (no square root at the denominator \\(2\\lambda_k\\mu_k\\)).\n\n\nWe can compute the conditional means, variances and covariances using reciprocal scaling scores. There are 2 equivalent formulas for that:\n\nusing the \\(H_k(i,j)\\) scores\nusing the CA coordinates\n\n\nUsing \\(H_k(i,j)\\)\n\nFor rows = sites\nWe compute the weighted mean of \\(H_k(i,j)\\) per row = site \\(i\\).\n\\[\nm_k(i) = \\frac{1}{y_{i\\cdot}} \\sum_{j = 1}^c y_{ij} H_k(i,j)\n\\]\n\n# Get marginal counts\nyi_ &lt;- rowSums(Y)\ny_j &lt;- colSums(Y)\n\n\n# Initialize mean vector\nmrows &lt;- matrix(nrow = r, \n                ncol = c-1) \n\nfor (i in 1:r) {\n  # Get nonzero cells for site i\n  rows &lt;- which(Yfreq0$row == i)\n  \n  # Get scores for site i\n  Hi &lt;- H[rows, ]\n  \n  # Get counts for site i\n  yij &lt;- Yfreq0$Freq[rows]\n  \n  # Fill i-th row with site i mean score along each axis\n  # (colSums sums all species j in site i)\n  if (is.matrix(Hi)) { # There are several species in site i\n    mrows[i, ] &lt;- 1/yi_[i]*colSums(diag(yij) %*% Hi)\n  } else { # Only one species in that site\n    mrows[i, ] &lt;- 1/yi_[i]*colSums(yij %*% Hi)\n  }\n  \n}\n\nWe compute the (weighted) variance of \\(H_k(i,j)\\) per row = site \\(i\\).\n\\[\ns_k^2(i) = \\frac{1}{y_{i\\cdot}} \\sum_{j = 1}^c y_{ij} \\left(H_k(i,j) - m_k(i)\\right)^2\n\\] NB: in the original article (Thioulouse and Chessel 1992) they use the compact variance formula \\(s_k^2(i) = \\frac{1}{y_{i\\cdot}} \\sum_{j = 1}^c \\left(y_{ij} H_k(i,j)^2\\right) - m_k(i)^2\\) but I prefer the other formula.\n\n# Initialize mean vector\nvarrows &lt;- matrix(nrow = r, \n                  ncol = c-1) \n\nfor (i in 1:r) {\n  # Get nonzero cells for site i\n  rows &lt;- which(Yfreq0$row == i)\n  \n  # Get scores for site i\n  Hi &lt;- H[rows, ]\n  \n  # Get counts for site i\n  yij &lt;- Yfreq0$Freq[rows]\n  \n  # Fill i-th row with site i variance along each axis\n  varrows[i, ] &lt;- 1/yi_[i]*colSums(diag(yij) %*% sweep(Hi, 2, mrows[i, ], \"-\")^2)\n}\n\nFinally, we can compute the covariance between scores on the several axes per row = site \\(i\\). The covariance between two axes \\(k\\) and \\(l\\) of the multivariate space for site = row \\(i\\) is defined as:\n\\[\nc_{kl}(i) = \\frac{1}{y_{i \\cdot}}\\sum_{j=1}^c y_{ij}H_k(i, j)H_l(i, j) - m_k(i)m_l(i)\n\\]\n\nk &lt;- 1\nl &lt;- 2\n\nBelow, we compute the covariance between axes \\(k =\\) 1 and \\(l =\\) 2.\n\n# Compute covariances for all sites\n\n# Create a df containing intermediate computation and row = site identifier\ndf &lt;- data.frame(var = Yfreq0$Freq*H[, k]*H[, l], \n                 by = Yfreq0$row)\n# Get the sum over the different species for each site\nsum_per_site &lt;- aggregate(df$var, list(df$by), sum)$x\n\n# Then vector-compute covariances\ncovrows &lt;- (1/yi_)*sum_per_site - mrows[, k]*mrows[, l]\n\n\n\nFor columns = species\nSymetrically, we can compute the means, variances and covariances per columns = species \\(j\\).\n\\[\nm_k(j) = \\frac{1}{y_{\\cdot j}} \\sum_{i = 1}^r y_{ij} H_k(i,j)\n\\]\n\\[\ns_k^2(j) = \\frac{1}{y_{\\cdot j}} \\sum_{i = 1}^r y_{ij} \\left(H_k(i,j) - m_k(j)\\right)^2\n\\]\n\\[c_{kl}(j) = \\frac{1}{y_{\\cdot j}}\\sum_{i=1}^r y_{ij}H_k(i, j)H_l(i, j) - m_k(j)m_l(j)\\]\n\n\nCode\n# Compute means and variances ---\n# Initialize mean vector\nmcols &lt;- matrix(nrow = c, \n                ncol = c-1) \nvarcols &lt;- matrix(nrow = c, \n                  ncol = c-1) \n\nfor (j in 1:c) {\n  # Get nonzero cells for species j\n  cols &lt;- which(Yfreq0$colind == j)\n  \n  # Get scores for species j\n  Hj &lt;- H[cols, ]\n  \n  # Get counts for species j\n  yij &lt;- Yfreq0$Freq[cols]\n  \n  # Fill j-th row with species j mean score/variance along each axis\n  if (is.matrix(Hj)) { # There are several sites with this species\n    mcols[j, ] &lt;- 1/y_j[j]*colSums(diag(yij) %*% Hj)\n    varcols[j, ] &lt;- 1/y_j[j]*colSums(diag(yij) %*% sweep(Hj, 2, mcols[j, ], \"-\")^2)\n  } else { # Only one site with that species\n    mcols[j, ] &lt;- 1/y_j[j]*colSums(yij %*% Hj)\n    varcols[j, ] &lt;- 1/y_j[j]*colSums(yij %*% (Hj - mcols[j, ])^2)\n  }\n  \n}\n\n\n\n\nCode\n# Compute covariances ---\n\n# Create a df containing intermediate computation and col = species identifier\ndf &lt;- data.frame(var = Yfreq0$Freq*H[, k]*H[, l], \n                 by = Yfreq0$colind)\n# Get the sum over the different sites for each species\nsum_per_spp &lt;- aggregate(df$var, list(df$by), sum)$x\n\n# Then vector-compute covariances\ncovcols &lt;- (1/y_j)*sum_per_spp - mcols[, k]*mcols[, l]\n\n\n\n\n\nUsing CA scores\n\nFor rows = sites\nThe conditional means and variances of rows scores on axis \\(k\\) for row \\(i\\) can also be computed as:\n\\[\nm_k(i) = \\frac{\\sqrt{\\mu_k}}{\\sqrt{2\\lambda_k}} L_k(i)\n\\]\n\\[\ns^2_k(i) = \\frac{1}{2\\lambda_k\\mu_k} \\left( \\frac{1}{y_{i \\cdot}} \\sum_{j = 1}^c \\left(y_{ij} C_k^2(j) \\right) - \\lambda_k L_k^2(i) \\right)\n\\] Note: this formula is different from formula (15) in Thioulouse and Chessel (1992) (no square root at the denominator \\(2\\lambda_k\\mu_k\\). Numerical computations suggest this is the correct formula.\n\nmrows2 &lt;- sweep(ca$li, 2, sqrt(mu)/sqrt(2*lambda), \"*\")\n\n\nvarrows2 &lt;- matrix(nrow = r, \n                   ncol = c-1)\n\nfor (i in 1:r) {\n  # Get CA scores for site i\n  Li &lt;- L[i, ]\n  \n  # Compute the part with the sum on Cj\n  # we add all coordinates Cj^2 weighted by the number of observations on site i\n  sumCj &lt;- t(Y[i, ]) %*% as.matrix(C)^2\n\n  # Fill i-th row with site i variance along each axis\n  varrows2[i, ] &lt;- 1/(2*lambda*mu)*((1/yi_[i])*sumCj - lambda*as.numeric(Li)^2)\n}\n\nWe can also compute the covariance using CA coordinates:\n\\[\nc_{kl}(i) = \\frac{1}{2\\sqrt{\\lambda_k \\lambda_l} \\sqrt{\\mu_k \\mu_l}} \\left( \\frac{1}{y_{i \\cdot}} \\sum_{j = 1}^c y_{ij} C_k(j) C_l(j) - \\sqrt{\\lambda_k \\lambda_l} L_k(i)L_l(i) \\right)\n\\]\nWe will use the same values as above for \\(k\\) and \\(l\\): \\(k =\\) 1 and \\(l =\\) 2.\n\n# Each site is multiplied by its score (Y*C[, k] by column) and then summed per site\nsum_per_site &lt;- rowSums(Y %*% diag(C[, k]) %*% diag(C[, l]))\n\ncovrows2 &lt;- 1/(2*sqrt(lambda[k]*lambda[l])*sqrt(mu[k]*mu[l]))*(1/yi_*sum_per_site - sqrt(lambda[k]*lambda[l])*L[, k]*L[, l])\n\n\n\nFor columns = species\nSame for the columns:\n\\[\nm_k(j) = \\frac{\\sqrt{\\mu_k}}{\\sqrt{2\\lambda_k}} C_k(j)\n\\]\n\\[\ns^2_k(j) = \\frac{1}{2\\lambda_k\\mu_k} \\left( \\frac{1}{y_{\\cdot j}} \\sum_{i = 1}^r \\left(y_{ij} L_k^2(i) \\right) - \\lambda_k C_k^2(j) \\right)\n\\]\n\\[\nc_{kl}(j) = \\frac{1}{2\\sqrt{\\lambda_k \\lambda_l} \\sqrt{\\mu_k \\mu_l}} \\left( \\frac{1}{y_{\\cdot j}} \\sum_{i = 1}^r y_{ij} L_k(i) L_l(i) - \\sqrt{\\lambda_k \\lambda_l} C_k(j)C_l(j) \\right)\n\\]\n\nmcols2 &lt;- sweep(ca$co, 2, sqrt(mu)/sqrt(2*lambda), \"*\")\n\n\nvarcols2 &lt;- matrix(nrow = c, \n                   ncol = c-1)\n\nfor (j in 1:c) {\n  # Get CA scores for species j\n  Cj &lt;- C[j, ]\n  \n  # Compute the part with the sum on Li\n  sumLi &lt;- t(Y[, j]) %*% as.matrix(L)^2\n\n  # Fill i-th row with site i variance along each axis\n  varcols2[j, ] &lt;- 1/(2*lambda*mu)*((1/y_j[j])*sumLi - lambda*as.numeric(Cj)^2)\n}\n\n\n# Each spp is multiplied by its score (Y*L[, k] by row) and then summed per spp\nsum_per_spp &lt;- colSums(diag(L[, k]) %*% diag(L[, l]) %*% Y)\n\ncovcols2 &lt;- 1/(2*sqrt(lambda[k]*lambda[l])*sqrt(mu[k]*mu[l]))*(1/y_j*sum_per_spp - sqrt(lambda[k]*lambda[l])*C[, k]*C[, l])\n\n\n\n\nPlots\nLet’s plot these means and variances along the first axis for rows and columns:\n\n\nCode\n# Define plotting function\nplot_meanvar &lt;- function(mean, var, groupnames, x = 1, col = \"black\")  {\n  ggplot() +\n  geom_point(aes(x = mean[, x], \n                 y = reorder(groupnames, -mean[, x])),\n             colour = col, shape = 1) +\n  geom_linerange(aes(x = mean[, x], \n                     y = reorder(groupnames, -mean[, x]),\n                     xmin = mean[, x] - var[, x],\n                     xmax = mean[, x] + var[, x]),\n                 colour = col) +\n  xlab(paste(\"Axis\", x)) +\n  theme_linedraw() +\n  theme(axis.title.y = element_blank())\n}\n\n\n\n\nCode\ngr &lt;- plot_meanvar(mrows, 3*sqrt(varrows), groupnames = rownames(Y),\n                   col = params$colsite) +\n  ggtitle(\"Rows (sites) mean and variance\")\ngc &lt;- plot_meanvar(mcols, 3*sqrt(varcols), groupnames = colnames(Y),\n                   col = params$colspp) +\n  ggtitle(\"Columns (species) mean and variance\")\n\ngrid.arrange(grobs = list(gr, gc), \n             nrow = 1)\n\n\n\n\n\n\n\nCode\ngr &lt;- ggplot() +\n  geom_point(aes(x = covrows, y = reorder(rownames(Y), -covrows)),\n             col = params$colsite) +\n  xlab(paste(\"Covariances between axes\", k, \"and\", l)) +\n  theme_linedraw() +\n  theme(axis.title.y = element_blank()) + \n  ggtitle(\"Rows (sites) covariances\")\n\ngc &lt;- ggplot() +\n  geom_point(aes(x = covcols, y = reorder(colnames(Y), -covcols)),\n             col = params$colspp) +\n  xlab(paste(\"Covariances between axes\", k, \"and\", l)) +\n  theme_linedraw() +\n  theme(axis.title.y = element_blank()) + \n  ggtitle(\"Columns (species) covariances\")\n\ngrid.arrange(grobs = list(gr, gc), nrow = 1)\n\n\n\n\n\nFinally, we can plot these ellipses in 2 dimensions and summarize:\n\nthe conditional mean along axes \\(k\\) and \\(l\\)\nthe conditional variance along axes \\(k\\) and \\(l\\)\nthe conditional covariances between axes \\(k\\) and \\(l\\)\n\nFor that, we need to plot the ellipse summarizing the Gaussian bivariate law for each group. For example, for site \\(i\\) on axes \\(k\\) and \\(l\\) reference blog post:\n\nthe mean vector is \\(\\mu(i) = (m_k(i), m_l(i))\\)\nthe variance-covariance matrix is \\(\\Sigma(i) = \\begin{pmatrix} \\sigma_k^2 ~ \\sigma_{kl}\\\\ \\sigma_{kl} ~ \\sigma_{l}^2 \\end{pmatrix}\\)\n\nThe simplest equation (coraviance zero, centered on the origin) is:\n\\[\\left( \\frac{x}{\\sigma_k} \\right)^2 + \\left( \\frac{y}{\\sigma_l} \\right)^2 = s\\] The semi-axes are of length \\(\\sigma_k \\sqrt{s}\\) and \\(\\sigma_l \\sqrt{s}\\). \\(s\\) is the scaling factor allowing the ellipsis to encompass all data with a level of confidence \\(p\\). We have \\(s = -2 \\ln(1-p)\\).\nWe can show that this equation is equivalent to\n\\[\\begin{cases} x = \\sigma_k \\sqrt{s} \\cos(t)\\\\ y = \\sigma_l \\sqrt{s} \\sin(t)  \\end{cases}\\] With \\(t\\) being an angle between 0 and \\(2\\pi\\).\nWe write \\(a\\) the length of the semi-major axis and \\(b\\) the length of the semi-minor axis.\nNow in case the values \\(H_k(i,j)\\) and \\(H_l(i,j)\\) are correlated, the covariance is not zero. Then, the semi-axes lengths are the square root of the eigenvaluesof \\(\\Sigma(i)\\). Geometrically, we have an angle \\(\\theta\\) between axis \\(k\\) and the semi-major axis. \\(\\theta\\) can be computed as the direction of the first eigenvector \\(v_1\\): \\(\\theta = \\arctan(v_1(2)/v_1(1))\\).\nThe equation of the ellipsis is: \\[\\begin{cases} x = m_k(i) +  a  \\cos(\\theta) \\cos(t) - b \\sin(\\theta) \\sin(t) \\\\ y = m_l(i) + a \\sin(\\theta) \\cos(t) + b \\cos(\\theta) \\sin(t) \\end{cases}\\] With \\(a = \\sqrt{\\lambda_1} \\sqrt{s}\\) and \\(b = \\sqrt{\\lambda_2} \\sqrt{s}\\). (Here we assumed the origin could be different from zero and shifted the center with \\(m_k(i)\\) and \\(m_l(i)\\).)\n\n\nCode\ngaussian_ellipses &lt;- function(vars, covars, means, x = 1, y = 2,\n                              t = seq(0, 2*pi, 0.01), s = 1.5, ind = 1:nrow(vars)) {\n  # Length of angles\n  nt &lt;- length(t)\n  \n  # Initialize matrix\n  ellipses_mat &lt;- matrix(nrow = length(t)*nrow(vars), \n                         ncol = 3)\n  \n  for (i in ind) {\n    # Define variance covariance matrix\n    covar &lt;- covars[i]\n    varx &lt;- vars[i, x]\n    vary &lt;- vars[i, y]\n    mat &lt;- matrix(data = c(varx, covar,\n                           covar, vary),\n                  nrow = 2)\n    \n    # Eigenvalues\n    vp &lt;- eigen(mat)$values\n    \n    a &lt;- s*sqrt(max(vp)) # semi major axis\n    b &lt;- s*sqrt(min(vp)) # semi minor axis\n    \n    # angle (inclination of ellipse from axe x)\n    if(covar == 0 & varx &gt;= vary){ # covariance is zero and varx is big\n      theta &lt;- 0\n    }else if(covar == 0 & varx &lt; vary){ # covariance is zero and varx is small\n      theta &lt;- pi/2\n    }else{ # \"normal\" cases\n      # get eigenvector associated to largest eigenvalue\n      v1 &lt;- eigen(mat)$vectors[,1]\n      # theta is the angle of the eigenvector with x -&gt; ie\n      # arctan of the slope given by the first eigenvector\n      theta &lt;- atan(v1[2]/v1[1])\n    }\n    \n    # x and y coordinates of ellipse\n    xellipse &lt;- means[i, x] + a*cos(theta)*cos(t) - b*sin(theta)*sin(t)\n    yellipse &lt;- means[i, y] + a*sin(theta)*cos(t) + b*cos(theta)*sin(t)\n    \n    indmin &lt;- (i-1)*nt + 1\n    indmax &lt;- i*nt\n    ellipses_mat[indmin:indmax, ] &lt;- matrix(c(xellipse, yellipse, rep(i,nt)),\n                                           ncol = 3)\n  }\n  res &lt;- as.data.frame(ellipses_mat)\n  colnames(res) &lt;- c(\"xell\", \"yell\", \"group\")\n  return(res)\n}\n\n\n\n\nCode\nplot_ellipses &lt;- function(var, covar, mean, groupnames, \n                          H, Yfreq0, col = \"black\", ellipses = 1:nrow(var), s = 1.5, x = 1, y = 2) {\n  \n  if (all(groupnames %in% Yfreq0$row)) {\n    indivtype &lt;- \"row\"\n  } else {\n    indivtype &lt;- \"colind\"\n  }\n  \n  rows &lt;- which(Yfreq0[[indivtype]] %in% ellipses)\n  \n  # Get gaussian ellipses for the required groups\n  ell &lt;- gaussian_ellipses(var, covar, mean, s = s, x = k, y = l, ind = ellipses)\n  \n  # Plot ellipses from variance and covariance\n  ggplot() +\n    geom_hline(yintercept = 0) +\n    geom_vline(xintercept = 0) +\n    geom_point(aes(x = H[, x], y = H[, y], alpha = Yfreq0[[indivtype]] %in% ellipses),\n               col = col,\n               show.legend = FALSE) +\n    scale_alpha_manual(values = c(\"TRUE\" = 1, \"FALSE\" = 0.2)) +\n    # Label ellipse points only\n    geom_text_repel(aes(x = H[rows, x], y = H[rows, y], \n                        label = paste(Yfreq0$row[rows], Yfreq0$col[rows])),\n                    col = col) +\n    geom_polygon(data = ell, \n                 aes(x = xell, y = yell, group = group),\n                 fill = col, col = col, alpha = 0.5) +\n    theme_linedraw() +\n    geom_label(aes(x = mean[, x], y = mean[, y], label = groupnames),\n               colour = col) +\n    xlab(paste(\"Axis\", x)) +\n    ylab(paste(\"Axis\", y)) \n}\n\n\n\n\nCode\ngr &lt;- plot_ellipses(varrows, covrows, mrows, groupnames = rownames(Y), H = H, Yfreq0 = Yfreq0,\n                    col = params$colsite) + ggtitle(\"Rows (sites)\")\n\ngc &lt;- plot_ellipses(varcols, covcols, mcols, groupnames = colnames(Y), H = H, Yfreq0 = Yfreq0, \n                    col = params$colspp) + ggtitle(\"Columns (species)\")\n\n\nWarning in sqrt(min(vp)): Production de NaN\n\n\nCode\ngrid.arrange(grobs = list(gr, gc))\n\n\nWarning: ggrepel: 178 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\nWarning: ggrepel: 179 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\nCode\ni &lt;- c(2, 13, 18)\ngr &lt;- plot_ellipses(varrows, covrows, mrows, groupnames = rownames(Y), H = H, Yfreq0 = Yfreq0,\n                    ellipses = i, col = params$colsite) + ggtitle(\"Rows (sites)\")\n\nj &lt;- c(16, 12, 20) # sp21 is index 20\ngc &lt;- plot_ellipses(varcols, covcols, mcols, groupnames = colnames(Y), H = H, Yfreq0 = Yfreq0, \n                    ellipses = j, col = params$colspp) + ggtitle(\"Columns (species)\")\n\ngrid.arrange(grobs = list(gr, gc))\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLets test formula (3) in Thioulouse and Chessel (1992) relating species and samples scores in CA.\nThe article says \\(L_k(i) = \\sum_{j=1}^c p_{j|i} C_k(j)\\) with \\(p_{j|i} = \\frac{y_{ij}}{y_{i \\cdot}}\\).\n\n# Test formula for a given k\nk &lt;- 2\n\n# compute pj|i\npjcondi &lt;- sweep(Y, 1, yi_, \"/\")\n\n# Compute the formula that should be equal th Lk(i)\nres &lt;- vector(mode=\"numeric\", length=nrow(Y))\nfor (i in 1:nrow(Y)) {\n  res[i] &lt;- sum(pjcondi[i, ]*ca$co[, k])\n}\n\nres/ca$li[ ,k] # not equal to Lk(i)\n\n [1] 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288\n [8] 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288\n[15] 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288\n[22] 0.4925288 0.4925288 0.4925288 0.4925288 0.4925288\n\nres_lambda &lt;- res*ca$eig[k]^(-1/2) # scale with 1/sqrt(lambda)\n\nres_lambda/ca$li[ ,k] # Now it is equal to Lk(i)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWe find that in fact the correct formula is\n\\[L_k(i) = \\frac{1}{\\sqrt{\\lambda_k}}\\sum_{j=1}^c p_{j|i} C_k(j)\\]\nWith this equality we can indeed prove numerically the equivalence between mean computation using \\(H_k(i,j)\\) (Equations (12) and (14) in Thioulouse and Chessel (1992)) and using CA coordinates (computation in notebook not shown here)."
  },
  {
    "objectID": "CCA.html",
    "href": "CCA.html",
    "title": "Canonical Correspondence Analysis (CCA)",
    "section": "",
    "text": "Code\n# Paths\nlibrary(here)\n\n# Multivariate analysis\nlibrary(ade4)\nlibrary(adegraphics)\n\n# Matrix algebra\nlibrary(expm)\n\n# Plots\nsource(here(\"functions/plot.R\"))\nlibrary(gridExtra)\nThe contents of this page relies heavily on (Legendre and Legendre 2012)."
  },
  {
    "objectID": "CCA.html#introduction",
    "href": "CCA.html#introduction",
    "title": "Canonical Correspondence Analysis (CCA)",
    "section": "Introduction",
    "text": "Introduction\nCCA is a method of the family of canonical or direct gradient analyses, in which a matrix of predictor variables intervenes in the computation of the ordination vectors.\nCCA is an asymmetric method because the predictor variables and the response variables are not equivalent in the analysis.\nCCA takes two matrices in input:\n\nA data matrix \\(Y\\) (\\(r \\times c\\))\nA matrix of predictor variables \\(E\\) (\\(r \\times l\\))\n\n\n\n\n\n\nHere, \\(Y\\) represents the abundance of different bird species (columns) at different sites (rows):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsp1\nsp2\nsp3\nsp4\nsp5\nsp6\nsp7\nsp8\nsp9\nsp10\nsp11\nsp12\nsp13\nsp14\nsp15\nsp16\nsp17\nsp18\nsp19\nsp21\nsp22\n\n\n\n\n1\n0\n1\n1\n0\n1\n0\n2\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n0\n0\n2\n0\n1\n0\n5\n0\n2\n0\n2\n4\n0\n3\n3\n0\n3\n1\n0\n2\n\n\n5\n0\n0\n2\n0\n1\n0\n2\n0\n1\n2\n0\n0\n0\n0\n0\n0\n3\n2\n0\n1\n\n\n3\n0\n0\n0\n0\n1\n0\n4\n1\n0\n1\n0\n0\n0\n0\n0\n0\n2\n0\n0\n1\n\n\n5\n0\n0\n1\n0\n1\n0\n2\n0\n0\n3\n0\n0\n0\n0\n1\n0\n3\n1\n0\n1\n\n\n1\n0\n0\n1\n0\n2\n0\n3\n1\n2\n1\n0\n0\n0\n1\n0\n1\n2\n2\n0\n3\n\n\n4\n0\n0\n0\n0\n2\n0\n4\n0\n2\n0\n4\n0\n0\n0\n0\n1\n5\n0\n0\n2\n\n\n2\n0\n0\n1\n0\n2\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n5\n1\n5\n1\n\n\n4\n0\n0\n0\n0\n0\n0\n2\n0\n0\n3\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n\n\n2\n0\n1\n5\n0\n0\n0\n5\n0\n1\n1\n4\n0\n0\n0\n1\n2\n2\n0\n0\n2\n\n\n2\n0\n0\n2\n0\n1\n0\n5\n0\n2\n2\n0\n0\n0\n4\n0\n1\n10\n3\n0\n1\n\n\n4\n0\n0\n0\n0\n1\n0\n4\n0\n2\n2\n4\n0\n0\n0\n0\n2\n1\n2\n0\n1\n\n\n5\n5\n0\n0\n1\n1\n0\n1\n0\n3\n0\n5\n0\n0\n0\n0\n0\n0\n1\n5\n1\n\n\n4\n0\n0\n1\n0\n2\n0\n0\n0\n1\n2\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n\n\n3\n0\n0\n0\n1\n0\n0\n0\n0\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n5\n0\n4\n0\n1\n2\n0\n0\n0\n0\n0\n0\n1\n2\n0\n0\n\n\n4\n0\n1\n1\n0\n1\n0\n3\n1\n2\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n\n\n5\n0\n0\n1\n0\n1\n0\n3\n2\n1\n2\n0\n0\n0\n0\n0\n0\n1\n3\n0\n0\n\n\n4\n2\n0\n1\n0\n0\n0\n2\n0\n2\n2\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n4\n0\n0\n1\n0\n2\n0\n2\n2\n2\n1\n1\n0\n0\n0\n0\n0\n2\n0\n0\n0\n\n\n5\n0\n0\n1\n0\n4\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n\n\n5\n1\n0\n3\n0\n2\n0\n2\n1\n0\n2\n0\n0\n0\n0\n0\n0\n3\n1\n0\n0\n\n\n3\n5\n0\n4\n0\n2\n0\n5\n0\n4\n2\n0\n0\n1\n0\n0\n0\n2\n0\n0\n0\n\n\n5\n2\n0\n7\n0\n1\n1\n0\n0\n4\n2\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n\n\n6\n0\n0\n2\n0\n1\n0\n0\n0\n1\n3\n0\n0\n0\n0\n0\n0\n0\n3\n3\n0\n\n\n3\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n\n\n\n\\(E\\) represents environmental variables (columns) associated to each site (rows).\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocation\nelevation\npatch_area\nperc_forests\nperc_grasslands\nShannonLandscapeDiv\n\n\n\n\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n0\n30\n7.92\n16.4129\n43.4066\n0.274\n\n\n0\n430\n83.24\n24.4526\n28.4995\n0.274\n\n\n0\n420\n140.83\n41.9966\n34.2412\n0.260\n\n\n0\n400\n140.83\n41.9966\n34.2412\n0.260\n\n\n0\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n0\n470\n7.84\n19.3087\n56.4031\n0.253\n\n\n0\n110\n21.15\n23.4668\n49.8908\n0.262\n\n\n0\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n0\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n0\n40\n6.66\n7.4729\n66.0061\n0.240\n\n\n0\n160\n5.63\n32.3516\n45.0477\n0.253\n\n\n0\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n1\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1\n30\n7.92\n16.4129\n43.4066\n0.274\n\n\n1\n430\n83.24\n24.4526\n28.4995\n0.274\n\n\n1\n420\n140.83\n41.9966\n34.2412\n0.260\n\n\n1\n400\n140.83\n41.9966\n34.2412\n0.260\n\n\n1\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n1\n470\n7.84\n19.3087\n56.4031\n0.253\n\n\n1\n110\n21.15\n23.4668\n49.8908\n0.262\n\n\n1\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n1\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n1\n40\n6.66\n7.4729\n66.0061\n0.240\n\n\n1\n160\n5.63\n32.3516\n45.0477\n0.253\n\n\n1\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n\n\n\n\n(r &lt;- dim(Y)[1])\n\n[1] 26\n\n(c &lt;- dim(Y)[2])\n\n[1] 21\n\n(l &lt;- dim(E)[2])\n\n[1] 6"
  },
  {
    "objectID": "CCA.html#computation",
    "href": "CCA.html#computation",
    "title": "Canonical Correspondence Analysis (CCA)",
    "section": "Computation",
    "text": "Computation\n\n\n\n\n\n\nTL;DR\n\n\n\nWe have a data matrix \\(Y\\) (\\(r \\times c\\)) and a matrix \\(E\\) (\\(r \\times l\\)) of predictors variables.\nWe regress \\(\\bar{Q}\\) (“centered” \\(Y\\)) on \\(E_{stand}\\) (which is the centered and scaled \\(E\\) matrix):\n\\[\n\\hat{\\bar{Q}} = D(p_{i\\cdot})^{1/2} E_{stand}B\n\\]\nThen, we diagonalize matrix \\(S_{\\hat{\\bar{Q}}'\\hat{\\bar{Q}}} = \\hat{\\bar{Q}}'\\hat{\\bar{Q}}\\).\n\\[\nS_{\\hat{\\bar{Q}}'\\hat{\\bar{Q}}} =  U \\Lambda U^{-1}\n\\]\nThe matrix \\(U\\) (\\(c \\times \\text{mindim}\\)) contains the loadings of the columns (species) of the contingency table. There are \\(\\text{mindim} = \\min(r-1, c, l)\\) non-null eigenvalues.\nThen, we can find the loadings rows (sites) \\(\\hat{U}\\) (\\(r \\times \\text{mindim}\\)) from the columns (species) loadings using the following formula:\n\\[\n\\hat{U} = \\bar{Q} U \\Lambda^{-1/2}\n\\]\n\n\n\nTransform matrix\nWe start by “centering” the matrix \\(Y\\) to get \\(\\bar{Q}\\) (as with CA).\nHere, we have:\n\n\nCode\nP &lt;- Y/sum(Y)\n\n# Initialize Qbar matrix\nQbar &lt;- matrix(ncol = ncol(Y), nrow = nrow(Y))\ncolnames(Qbar) &lt;- colnames(Y)\nrownames(Qbar) &lt;- rownames(Y)\n\nfor(i in 1:nrow(Y)) { # For each row\n  for (j in 1:ncol(Y)) { # For each column\n    # Do the sum\n    pi_ &lt;- sum(P[i, ])\n    p_j &lt;- sum(P[, j])\n    \n    # Compute the transformation\n    Qbar[i, j] &lt;- (P[i, j] - (pi_*p_j))/sqrt(pi_*p_j)\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsp1\nsp2\nsp3\nsp4\nsp5\nsp6\nsp7\nsp8\nsp9\nsp10\nsp11\nsp12\nsp13\nsp14\nsp15\nsp16\nsp17\nsp18\nsp19\nsp21\nsp22\n\n\n\n\n-0.0246174\n-0.0235678\n0.1819102\n0.0177849\n-0.0086058\n0.0190444\n-0.0060852\n0.0389306\n-0.0172115\n0.0177849\n0.0142185\n-0.0285421\n-0.0121704\n-0.0086058\n-0.0172115\n-0.0136069\n-0.0172115\n-0.0425963\n0.0259875\n-0.0227687\n-0.0250899\n\n\n-0.0700532\n-0.0430288\n-0.0192431\n-0.0075494\n-0.0157119\n-0.0362309\n-0.0111100\n0.0317935\n-0.0314238\n-0.0075494\n-0.0702658\n0.0257394\n0.3429284\n-0.0157119\n0.1622254\n0.2201063\n-0.0314238\n0.0004761\n-0.0290665\n-0.0415698\n0.0427538\n\n\n0.0325898\n-0.0342433\n-0.0153141\n0.0216502\n-0.0125039\n-0.0148135\n-0.0088416\n-0.0092517\n-0.0250078\n-0.0160655\n0.0166286\n-0.0414707\n-0.0176832\n-0.0125039\n-0.0250078\n-0.0197704\n-0.0250078\n0.0364300\n0.0331807\n-0.0330822\n0.0191867\n\n\n0.0149125\n-0.0283250\n-0.0126673\n-0.0444862\n-0.0103428\n0.0023441\n-0.0073135\n0.0865729\n0.0773724\n-0.0444862\n-0.0024017\n-0.0343033\n-0.0146270\n-0.0103428\n-0.0206857\n-0.0163535\n-0.0206857\n0.0280484\n-0.0407198\n-0.0273646\n0.0371130\n\n\n0.0381182\n-0.0333300\n-0.0149056\n-0.0135976\n-0.0121704\n-0.0123509\n-0.0086058\n-0.0058019\n-0.0243408\n-0.0523468\n0.0573758\n-0.0403646\n-0.0172115\n-0.0121704\n-0.0243408\n0.0861662\n-0.0243408\n0.0407749\n-0.0055815\n-0.0321998\n0.0216837\n\n\n-0.0648859\n-0.0351329\n-0.0157119\n-0.0184176\n-0.0128287\n0.0201080\n-0.0090713\n0.0163368\n0.0533995\n0.0183431\n-0.0220164\n-0.0425480\n-0.0181425\n-0.0128287\n0.0533995\n-0.0202840\n0.0533995\n0.0003888\n0.0298152\n-0.0339416\n0.1252960\n\n\n-0.0121284\n-0.0384861\n-0.0172115\n-0.0604449\n-0.0140532\n0.0084189\n-0.0099371\n0.0284370\n-0.0281063\n0.0066707\n-0.0628476\n0.1274686\n-0.0198742\n-0.0140532\n-0.0281063\n-0.0222200\n0.0440625\n0.0762434\n-0.0553273\n-0.0371811\n0.0580431\n\n\n-0.0383974\n-0.0342433\n-0.0153141\n-0.0160655\n-0.0125039\n0.0234224\n-0.0088416\n-0.0684866\n-0.0250078\n-0.0537812\n-0.0196452\n0.0074409\n-0.0176832\n-0.0125039\n-0.0250078\n-0.0197704\n-0.0250078\n0.1019773\n-0.0080236\n0.2734875\n0.0191867\n\n\n0.0509731\n-0.0272138\n-0.0121704\n-0.0427410\n-0.0099371\n-0.0421595\n-0.0070266\n0.0201080\n-0.0198742\n-0.0427410\n0.0924907\n-0.0329576\n-0.0140532\n-0.0099371\n-0.0198742\n-0.0157119\n0.0821879\n-0.0079467\n-0.0391223\n-0.0262910\n0.0410427\n\n\n-0.0598219\n-0.0400576\n0.0953134\n0.0982934\n-0.0146270\n-0.0620570\n-0.0103428\n0.0464771\n-0.0292540\n-0.0306717\n-0.0344052\n0.1187362\n-0.0206857\n-0.0146270\n-0.0292540\n0.0645785\n0.1094211\n-0.0163667\n-0.0575865\n-0.0386994\n0.0524857\n\n\n-0.0770633\n-0.0451290\n-0.0201823\n-0.0136415\n-0.0164788\n-0.0409006\n-0.0116523\n0.0221086\n-0.0329576\n-0.0136415\n-0.0186472\n-0.0546539\n-0.0233045\n-0.0164788\n0.2132254\n-0.0260552\n0.0285882\n0.1671166\n0.0289188\n-0.0435988\n-0.0058235\n\n\n-0.0082886\n-0.0376758\n-0.0168491\n-0.0591722\n-0.0137573\n-0.0236147\n-0.0097279\n0.0323248\n-0.0275145\n0.0093869\n0.0044137\n0.1321940\n-0.0194557\n-0.0137573\n-0.0275145\n-0.0217521\n0.1199274\n-0.0383072\n0.0207382\n-0.0363983\n0.0104632\n\n\n-0.0066028\n0.2024052\n-0.0185906\n-0.0652879\n0.1184515\n-0.0329026\n-0.0107333\n-0.0587421\n-0.0303583\n0.0279175\n-0.0678832\n0.1511122\n-0.0214665\n-0.0151791\n-0.0303583\n-0.0240003\n-0.0303583\n-0.0751329\n-0.0258181\n0.2123779\n0.0015805\n\n\n0.0509731\n-0.0272138\n-0.0121704\n0.0047169\n-0.0099371\n0.0540656\n-0.0070266\n-0.0544276\n-0.0198742\n0.0047169\n0.0468471\n-0.0329576\n-0.0140532\n-0.0099371\n-0.0198742\n-0.0157119\n-0.0198742\n-0.0491860\n0.0645729\n-0.0262910\n-0.0289713\n\n\n0.0537748\n-0.0222200\n-0.0099371\n-0.0348979\n0.2418864\n-0.0344230\n-0.0057372\n-0.0444400\n-0.0162272\n0.1394736\n-0.0362851\n0.0484681\n-0.0114743\n-0.0081136\n-0.0162272\n-0.0128287\n-0.0162272\n-0.0401602\n-0.0319432\n-0.0214665\n-0.0236550\n\n\n-0.0105035\n-0.0333300\n-0.0149056\n-0.0523468\n-0.0121704\n0.1447840\n-0.0086058\n0.0550562\n-0.0243408\n-0.0135976\n0.0201080\n-0.0403646\n-0.0172115\n-0.0121704\n-0.0243408\n-0.0192431\n-0.0243408\n-0.0265686\n0.0367519\n-0.0321998\n-0.0354825\n\n\n0.0303585\n-0.0304260\n0.1354643\n-0.0053382\n-0.0111100\n-0.0041026\n-0.0078559\n0.0391481\n0.0690671\n0.0371094\n-0.0496854\n-0.0368477\n-0.0157119\n-0.0111100\n-0.0222200\n-0.0175664\n-0.0222200\n0.0187795\n-0.0437401\n-0.0293943\n-0.0323909\n\n\n0.0325898\n-0.0342433\n-0.0153141\n-0.0160655\n-0.0125039\n-0.0148135\n-0.0088416\n0.0203657\n0.1372137\n-0.0160655\n0.0166286\n-0.0414707\n-0.0176832\n-0.0125039\n-0.0250078\n-0.0197704\n-0.0250078\n-0.0291174\n0.0743850\n-0.0330822\n-0.0364548\n\n\n0.0244779\n0.0976756\n-0.0140532\n-0.0082533\n-0.0114743\n-0.0486815\n-0.0081136\n0.0017021\n-0.0229487\n0.0328465\n0.0277421\n-0.0380561\n-0.0162272\n0.1653023\n-0.0229487\n-0.0181425\n-0.0229487\n-0.0210808\n-0.0002732\n-0.0303583\n-0.0334532\n\n\n0.0189774\n-0.0323909\n-0.0144857\n-0.0109993\n-0.0118275\n0.0306654\n-0.0083633\n-0.0021594\n0.1478436\n0.0288733\n-0.0145459\n0.0124814\n-0.0167266\n-0.0118275\n-0.0236550\n-0.0187009\n-0.0236550\n0.0107528\n-0.0465649\n-0.0312926\n-0.0344828\n\n\n0.0502635\n-0.0314238\n-0.0140532\n-0.0082533\n-0.0114743\n0.1179851\n-0.0081136\n-0.0628476\n-0.0229487\n-0.0493530\n0.1067990\n-0.0380561\n-0.0162272\n-0.0114743\n-0.0229487\n-0.0181425\n-0.0229487\n-0.0567951\n0.0446281\n-0.0303583\n-0.0334532\n\n\n0.0273672\n0.0226022\n-0.0157119\n0.0551038\n-0.0128287\n0.0201080\n-0.0090713\n-0.0125307\n0.0533995\n-0.0551784\n0.0133389\n-0.0425480\n-0.0181425\n-0.0128287\n-0.0256574\n-0.0202840\n-0.0256574\n0.0323326\n-0.0103457\n-0.0339416\n-0.0374018\n\n\n-0.0455868\n0.2024052\n-0.0185906\n0.0589860\n-0.0151791\n-0.0014055\n-0.0107333\n0.0388479\n-0.0303583\n0.0589860\n-0.0081217\n-0.0503435\n-0.0214665\n0.1184515\n-0.0303583\n-0.0240003\n-0.0303583\n-0.0211380\n-0.0597603\n-0.0401602\n-0.0442544\n\n\n0.0089254\n0.0669231\n-0.0172115\n0.1744597\n-0.0140532\n-0.0256018\n0.1941871\n-0.0769723\n-0.0281063\n0.0737863\n0.0017021\n-0.0466090\n-0.0198742\n-0.0140532\n-0.0281063\n-0.0222200\n-0.0281063\n-0.0695595\n0.0179962\n-0.0371811\n-0.0409716\n\n\n0.0562523\n-0.0342433\n-0.0153141\n0.0216502\n-0.0125039\n-0.0148135\n-0.0088416\n-0.0684866\n-0.0250078\n-0.0160655\n0.0529024\n-0.0414707\n-0.0176832\n-0.0125039\n-0.0250078\n-0.0197704\n-0.0250078\n-0.0618911\n0.0743850\n0.1508596\n-0.0364548\n\n\n0.0649207\n-0.0207849\n-0.0092953\n-0.0326440\n-0.0075896\n0.0307943\n-0.0053666\n-0.0415698\n-0.0151791\n-0.0326440\n0.0258198\n-0.0251718\n-0.0107333\n-0.0075896\n-0.0151791\n-0.0120002\n-0.0151791\n-0.0375664\n0.0380043\n0.0809351\n-0.0221272\n\n\n\n\n\n\n\nWeights on \\(E\\)\nWe standardize \\(E\\) as \\(E_{stand}\\), but we standardize so that the variables associated to the sites which have more observations have more weight.\nTo do that, we use the inflated matrix \\(E_{infl}\\) to compute its mean \\(\\bar{E}_{infl}\\) and standard deviation \\(\\sigma(E_{infl})\\) per column. In \\(E_{infl}\\), the rows corresponding to each site are duplicates as many times as there are observations for this given site (so that \\(E_{infl}\\) is of dimension \\(y_{\\cdot \\cdot} \\times l\\)).\n\\[\nE_{stand} = \\frac{E - \\bar{E}_{infl}}{\\sigma(E_{infl})}\n\\]\nWith our data: first, we compute \\(E_{infl}\\):\n\n\nCode\nEinfl &lt;- matrix(data = 0,\n                ncol = ncol(E), \n                nrow = sum(Y))\ncolnames(Einfl) &lt;- colnames(E)\nrownames(Einfl) &lt;- 1:nrow(Einfl)\n\n# Get the number of times to duplicate each site\nnrep_sites &lt;- rowSums(Y)\n\nnrstart &lt;- 0\nfor (i in 1:nrow(E)) {\n  ri &lt;- as.matrix(E)[i, ]\n  \n  rname &lt;- rownames(E)[i]\n  \n  # Get how many times to duplicate this site\n  nr &lt;- nrep_sites[as.character(rname)]\n  \n  Einfl[(nrstart+1):(nrstart+nr), ] &lt;- matrix(rep(ri, nr), \n                                              nrow = nr, \n                                              byrow = TRUE)\n  \n  rownames(Einfl)[(nrstart+1):(nrstart+nr)] &lt;- paste(rname,\n                                                     1:nr,\n                                                     sep = \"_\")\n  nrstart &lt;- nrstart + nr\n}\npaste(\"Dimension:\", paste(dim(Einfl), collapse = \" \"))\n\n\n[1] \"Dimension: 493 6\"\n\n\nCode\nknitr::kable(head(Einfl, 10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocation\nelevation\npatch_area\nperc_forests\nperc_grasslands\nShannonLandscapeDiv\n\n\n\n\n1_1\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1_2\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1_3\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1_4\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1_5\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1_6\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1_7\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1_8\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1_9\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n2_1\n0\n30\n7.92\n16.4129\n43.4066\n0.274\n\n\n\n\n\nThen, we compute the mean and standard deviation per column and standardize \\(E\\).\n\nEinfl_mean &lt;- apply(Einfl, 2, mean)\n\nn &lt;- nrow(Einfl)\nEinfl_sd &lt;- apply(Einfl, 2, \n                  function(x) sd(x)*sqrt((n-1)/n))\n\n\nEstand &lt;- scale(E, \n                center = Einfl_mean,\n                scale = Einfl_sd)\nknitr::kable(Estand)\n\n\n\n\n\n\n\n\n\n\n\n\nlocation\nelevation\npatch_area\nperc_forests\nperc_grasslands\nShannonLandscapeDiv\n\n\n\n\n-0.8940191\n-1.4578384\n-0.5429520\n-1.0404731\n1.0369741\n-1.0351817\n\n\n-0.8940191\n-1.3502706\n-0.5081483\n-0.3311255\n-0.4345644\n1.0954816\n\n\n-0.8940191\n0.8010856\n1.0902771\n0.3301082\n-1.3346326\n1.0954816\n\n\n-0.8940191\n0.7473017\n2.3124400\n1.7730331\n-0.9879574\n0.3852605\n\n\n-0.8940191\n0.6397339\n2.3124400\n1.7730331\n-0.9879574\n0.3852605\n\n\n-0.8940191\n1.1775730\n-0.6656140\n-1.0605165\n0.9946790\n-0.6293411\n\n\n-0.8940191\n1.0162212\n-0.5098460\n-0.0929573\n0.3501447\n0.0301499\n\n\n-0.8940191\n-0.9199993\n-0.2273840\n0.2490300\n-0.0430582\n0.4867206\n\n\n-0.8940191\n0.9624373\n-0.1495000\n0.0784846\n0.3887809\n-0.4771509\n\n\n-0.8940191\n0.9086534\n-0.6102251\n-1.2838968\n1.3976330\n-1.8468630\n\n\n-0.8940191\n-1.2964867\n-0.5348877\n-1.0664053\n0.9299593\n-0.6293411\n\n\n-0.8940191\n-0.6510798\n-0.5567462\n0.9797698\n-0.3354772\n0.0301499\n\n\n-0.8940191\n-0.6510798\n-0.4429974\n0.7628294\n-1.9186435\n1.9578929\n\n\n1.1185444\n-1.4578384\n-0.5429520\n-1.0404731\n1.0369741\n-1.0351817\n\n\n1.1185444\n-1.3502706\n-0.5081483\n-0.3311255\n-0.4345644\n1.0954816\n\n\n1.1185444\n0.8010856\n1.0902771\n0.3301082\n-1.3346326\n1.0954816\n\n\n1.1185444\n0.7473017\n2.3124400\n1.7730331\n-0.9879574\n0.3852605\n\n\n1.1185444\n0.6397339\n2.3124400\n1.7730331\n-0.9879574\n0.3852605\n\n\n1.1185444\n1.1775730\n-0.6656140\n-1.0605165\n0.9946790\n-0.6293411\n\n\n1.1185444\n1.0162212\n-0.5098460\n-0.0929573\n0.3501447\n0.0301499\n\n\n1.1185444\n-0.9199993\n-0.2273840\n0.2490300\n-0.0430582\n0.4867206\n\n\n1.1185444\n0.9624373\n-0.1495000\n0.0784846\n0.3887809\n-0.4771509\n\n\n1.1185444\n0.9086534\n-0.6102251\n-1.2838968\n1.3976330\n-1.8468630\n\n\n1.1185444\n-1.2964867\n-0.5348877\n-1.0664053\n0.9299593\n-0.6293411\n\n\n1.1185444\n-0.6510798\n-0.5567462\n0.9797698\n-0.3354772\n0.0301499\n\n\n1.1185444\n-0.6510798\n-0.4429974\n0.7628294\n-1.9186435\n1.9578929\n\n\n\n\n\n\n\nRegression\nAfter that, we perform the weighted multiple linear regression of \\(\\bar{Q}\\) by \\(E_{stand}\\). Each row of \\(\\bar{Q}\\) \\(\\bar{q}\\) is approximated by \\(\\hat{\\bar{q}}\\) using multiple linear regression on \\(E_{stand}\\): \\(\\hat{\\bar{q}} = b_0 + b_1 e_{1} + \\ldots + b_e e_{l}\\), where \\(e_i\\) are columns of \\(E_{stand}\\) and \\(b_i\\) are regression coefficients. In a matrix form, it is written:\n\\[\n\\hat{\\bar{Q}} = D(p_{i\\cdot})^{1/2}E_{stand}B\n\\]\nWhere\n\\[\nB = [E_{stand}' D(p_{i\\cdot}) E_{stand}]^{-1}E_{stand}'D(p_{i\\cdot})^{1/2}\\bar{Q}\n\\]\nWith our example:\n\n# Diagonal matrix weights\nDpi_ &lt;-  diag(rowSums(P))\ncolnames(Dpi_) &lt;- rownames(P)\n\n# Regression coefficient\nB &lt;- solve(t(Estand) %*% Dpi_ %*% Estand) %*% t(Estand) %*% sqrt(Dpi_) %*% Qbar\n\nNow, we can get the predicted values for \\(\\bar{Q}\\). For instance, let’s plot predicted vs observed values for site 2:\n\n# Compute predicted values\nQbarhat &lt;- sqrt(Dpi_) %*% Estand %*% B\ncolnames(Qbarhat) &lt;- colnames(Qbar)\nrownames(Qbarhat) &lt;- rownames(Qbar)\ndim(Qbarhat)\n\n[1] 26 21\n\n# Get predicted and observed values for one site\nind &lt;- 2\n\npred &lt;- Qbarhat[ind, ]\nobs &lt;- Qbar[ind, ]\n\n\n\nCode\ndf &lt;- data.frame(pred, obs, names = names(pred))\ndf&lt;- df |&gt; \n  tidyr::pivot_longer(cols = c(\"pred\", \"obs\"))\n\nggplot(df, aes(x = names, y = value, col = name)) +\n  geom_point() +\n  theme_linedraw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\nDiagonalization\nThen, we diagonalize the covariance matrix of predicted values \\(S_{\\hat{\\bar{Q}}' \\hat{\\bar{Q}}}\\).\nFirst, we compute the covariance matrix (note that here we don’t divide by the degrees of freedom).\n\\[\nS_{\\hat{\\bar{Q}}' \\hat{\\bar{Q}}} = \\hat{\\bar{Q}}' \\hat{\\bar{Q}}\n\\]\n\nSqq &lt;- t(Qbarhat) %*% Qbarhat\n\nWe diagonalize \\(S_{\\hat{\\bar{Q}}' \\hat{\\bar{Q}}}\\):\n\\[\nS_{\\hat{\\bar{Q}}' \\hat{\\bar{Q}}} = U \\Lambda U^{-1}\n\\]\n\\(\\Lambda\\) is the matrix of eigenvalues (there are \\(\\min(r-1, c, l)\\) non-null eigenvalues) and \\(U\\) contains the columns (species) loadings.\n\nneig &lt;- min(c(r-1, c, l))\n\nHere, \\(r-1 =\\) 25, \\(c =\\) 21 and \\(l =\\) 6 so the minimum (and the number of eigenvalues) is 6.\n\neig &lt;- eigen(Sqq)\n\n(lambda &lt;- eig$values) # There are l = 6 non-null eigenvalues\n\n [1]  1.846574e-01  1.571586e-01  1.009501e-01  8.711508e-02  3.222443e-02\n [6]  1.476469e-02  9.795705e-18  8.252762e-18  7.486518e-18  6.959888e-18\n[11]  5.759585e-18  5.381418e-18  4.061072e-18  1.605351e-18  4.678542e-19\n[16]  3.679331e-19 -4.523252e-20 -7.096450e-19 -4.140284e-18 -7.864342e-18\n[21] -2.116046e-17\n\nlambda &lt;- lambda[1:neig]\n\nLambda &lt;- diag(lambda)\nU &lt;- eig$vectors[, 1:neig] # We keep only the eigenvectors\nrownames(U) &lt;- colnames(Y)\n\nFinally, we can get the ordination of rows (sites) by using the link between rows and columns ordination:\n\\[\n\\hat{U} = \\bar{Q} U \\Lambda^{-1/2}\n\\]\n\nUhat &lt;- Qbar %*% U %*% diag(lambda^(-1/2))\nrownames(Uhat) &lt;- rownames(Qbar)\n\ndim(Uhat)\n\n[1] 26  6\n\n\nNB: this is not the same as diagonalizing S_{ ’} or performing the SVD of \\(\\hat{\\bar{Q}}\\), because $ does not contain the loadings of \\(\\hat{\\bar{Q}}\\) but the loadings of \\(\\bar{Q}\\)."
  },
  {
    "objectID": "CCA.html#scaling",
    "href": "CCA.html#scaling",
    "title": "Canonical Correspondence Analysis (CCA)",
    "section": "Scaling",
    "text": "Scaling\n\n\n\n\n\n\nTL;DR\n\n\n\nAs with CA, we have scalings type 1, 2 and 3. There are 2 differences with CA:\n\nFor the sites, we have 2 types of scores: the predicted sites scores (LC scores), computed from the regression, and the weighted averages (WA scores), computed from the species.\nThere are also scalings for the correlations of the explanatory variables with the axes.\n\nIt seems that the general formulas for scalings are:\n\nWA scores for rows (sites): \\(F = D(p_{\\cdot j})^{-1/2}\\hat{U} \\Lambda^{\\alpha/2}\\)\nLC scores for rows (sites): \\(Z_1 = D(p_{i\\cdot})^{-1/2}\\hat{\\bar{Q}}U \\Lambda^{?}\\)\nColumns (species) scores: \\(V = D(p_{\\cdot j})^{-1/2}U \\Lambda^{(1-\\alpha)/2}\\)\nVariables scores: \\(BS_1 = E_{stand}' D(p_{i\\cdot})Z_{stand} \\Lambda^{\\alpha/2}\\)\n\nwith \\(\\alpha =\\) 1, 0 or 1/2.\nScaling type 1 (\\(\\alpha = 1\\))\n\nWA scores for rows (sites): \\(F = D(p_{\\cdot j})^{-1/2}\\hat{U} \\Lambda^{1/2}\\)\nLC scores for rows (sites): \\(Z_1 = D(p_{i\\cdot})^{-1/2}\\hat{\\bar{Q}}U\\)\nColumns (species) scores: \\(V = D(p_{\\cdot j})^{-1/2}U\\)\nVariables scores: \\(BS_1 = E_{stand}' D(p_{i\\cdot})Z_{stand}\\Lambda^{1/2}\\)\n\nScaling type 2 (\\(\\alpha = 0\\))\n\nWA scores for rows (sites): \\(\\hat{V} = D(p_{\\cdot j})^{-1/2}\\hat{U}\\)\nLC scores for rows (sites): \\(Z_2 = D(p_{i\\cdot})^{-1/2}\\hat{\\bar{Q}} U \\Lambda^{-1/2}\\)\nColumns (species) scores: \\(\\hat{F} = D(p_{\\cdot j})^{-1/2}U \\Lambda^{1/2}\\)\nVariables scores: \\(BS_2 = E_{stand}' D(p_{i\\cdot})Z_{stand}\\)\n\nScaling type 3 (\\(\\alpha = 1/2\\))\n\nWA scores for rows (sites): \\(\\hat{S}_3 = D(p_{\\cdot j})^{-1/2}\\hat{U} \\Lambda^{1/4}\\)\nLC scores for rows (sites): \\(Z_3 = D(p_{i\\cdot})^{-1/2}\\hat{\\bar{Q}}U \\Lambda^{-1/4}\\)\nColumns (species) scores: \\(S_3 = D(p_{\\cdot j})^{-1/2} U \\Lambda^{1/4}\\)\nVariables scores: \\(BS_3 = E_{stand}' D(p_{i\\cdot})Z_{stand}\\Lambda^{1/4}\\)\n\nHow to find the fa (ie correlation coefficients for each environmental variable in the multivariate space)??\n\n\nLet’s compute CCA with ade4 to compare results.\n\nca &lt;- dudi.coa(Y, \n               nf = c-1, \n               scannf = FALSE)\ncca &lt;- pcaiv(dudi = ca, \n             df = E,\n             scannf = FALSE,\n             nf = neig)\n\nlambda[1:neig]/cca$eig # Eigenvalues are the same as computed manually\n\n[1] 1 1 1 1 1 1\n\n\n\nPre-computations\nWe define the diagonal matrices with rows and columns weights (respectively \\(D(p_{i\\cdot}\\) and \\(D(p_{\\cdot j}\\)).\n\n# Define weights matrices\nDpi_ &lt;- diag(rowSums(P))\nDp_j &lt;- diag(colSums(P))\n\n# Also define a variable for just the sums for easier manipulation\ndpi_ &lt;- rowSums(P)\ndp_j &lt;- colSums(P)\n\nThe scores for the explanatory variables are computed from matrix \\(Z_{stand}\\): so we first need to standardize \\(Z\\) into \\(Z_{stand}\\) (the computation can involve \\(Z_1\\), \\(Z_2\\) or \\(Z_3\\) and give the same result).\n\n\nCode\n# Compute Z1\nZ1 &lt;- diag(dpi_^(-1/2)) %*% Qbarhat %*% U\n\n# Compute inflated matrix ------\nZinfl &lt;- matrix(data = 0,\n                ncol = ncol(Z1), \n                nrow = sum(Y))\nrownames(Zinfl) &lt;- 1:nrow(Zinfl)\n# Give names to Z1\nrownames(Z1) &lt;- rownames(Y)\n\n# Get the number of times to duplicate each site\nnrep_sites &lt;- rowSums(Y)\n\nnrstart &lt;- 0\nfor (i in 1:nrow(Z1)) {\n  ri &lt;- Z1[i, ]\n  \n  rname &lt;- rownames(Z1)[i]\n  \n  # Get how many times to duplicate this site\n  nr &lt;- nrep_sites[as.character(rname)]\n  \n  Zinfl[(nrstart+1):(nrstart+nr), ] &lt;- matrix(rep(ri, nr), \n                                              nrow = nr, \n                                              byrow = TRUE)\n  \n  rownames(Zinfl)[(nrstart+1):(nrstart+nr)] &lt;- paste(rname, \n                                                     1:nr, \n                                                     sep = \"_\")\n  nrstart &lt;- nrstart + nr\n}\n\npaste(\"Dimension of Zinfl:\", \n      paste(dim(Zinfl), collapse = \" \"))\n\n\n[1] \"Dimension of Zinfl: 493 6\"\n\n\nCode\n# Compute mean/sd ------\nZinfl_mean &lt;- apply(Zinfl, 2, mean)\n\nn &lt;- nrow(Zinfl)\nZinfl_sd &lt;- apply(Zinfl, 2, \n                  function(x) sd(x)*sqrt((n-1)/n))\n\n# Compute Zstand ------\nZstand &lt;- scale(Z1, \n                center = Zinfl_mean,\n                scale = Zinfl_sd)\n\npaste(\"Dimension of Zstand:\", \n      paste(dim(Zstand), collapse = \" \"))\n\n\n[1] \"Dimension of Zstand: 26 6\"\n\n\n\n\nScaling type 1\nHere, \\(\\chi^2\\) distances between rows (sites) are preserved they are positioned at the centroid of species.\n\nthe rows (sites) scores can be \\(F = D(p_{i \\cdot})^{-1/2} \\hat{U} \\Lambda^{1/2}\\) or \\(Z_1 = D(p_{i\\cdot})^{-1/2} \\hat{\\bar{Q}} U\\)\nthe columns (species) scores are \\(V = D(p_{\\cdot j})^{-1/2} U\\)\nthe explanatory variable score are \\(BS_1 = E_{stand}' D(p_{i\\cdot})Z_{stand}\\Lambda^{1/2}\\)\n\n\n# Row scores\nF_ &lt;- diag(dpi_^(-1/2)) %*% Uhat %*% Lambda^(1/2)\nZ1 &lt;- diag(dpi_^(-1/2)) %*% Qbarhat %*% U\n\n# Columsn scores\nV &lt;- diag(dp_j^(-1/2)) %*% U\n\n# Variables correlation\nBS1 &lt;- t(Estand) %*% Dpi_ %*% Zstand %*% sqrt(Lambda)\n\n\n\nCode\nmult &lt;- 10\n# WA scores\ngwa &lt;- multiplot(indiv_row = F_, indiv_col = V, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS1, var_row_lab = colnames(E),\n                 row_color = params$colsite, col_color = params$colspp,\n                 mult = mult,\n                 eig = lambda) +\n  ggtitle(\"WA scores for sites\")\n\n# LC scores\nglc &lt;- multiplot(indiv_row = Z1, indiv_col = V, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS1, var_row_lab = colnames(E),\n                 row_color = params$colsite, col_color = params$colspp,\n                 mult = mult,\n                 eig = lambda) +\n  ggtitle(\"LC scores for sites\")\n\ngrid.arrange(grobs = list(gwa, glc),\n             nrow = 1)\n\n\n\n\n\nThe plot with the predicted sites scores has the same coordinates for sites as with ade4.\n\n\nScaling type 2\nHere, \\(\\chi^2\\) distances between columns (species) are preserved they are positioned at the centroid of sites.\n\nthe rows (sites) scores can be \\(\\hat{V} = D(p_{i \\cdot})^{-1/2} \\hat{U}\\) or \\(Z_2 = D(p_{i\\cdot})^{-1/2} \\hat{\\bar{Q}} U \\Lambda^{-1/2}\\)\nthe columns (species) scores are \\(\\hat{F} = D(p_{\\cdot j})^{-1/2} U \\Lambda^{1/2}\\)\nthe explanatory variable score are \\(BS_2 = E_{stand}' D(p_{i\\cdot})Z_{stand}\\)\n\n\n# Row scores\nVhat &lt;- diag(dpi_^(-1/2)) %*% Uhat\nZ2 &lt;- diag(dpi_^(-1/2)) %*% Qbarhat %*% U %*% diag(lambda^(-1/2))\n\n# Columsn scores\nFhat &lt;- diag(dp_j^(-1/2)) %*% U %*% Lambda^(1/2)\n\n# Variables correlation\nBS2 &lt;- t(Estand) %*% Dpi_ %*% Zstand\n\nOn these graphs, species are in the same position as with ade4.\n\n\nCode\nmult &lt;- 5\n# WA scores\ngwa &lt;- multiplot(indiv_row = Vhat, indiv_col = Fhat, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS2, var_row_lab = colnames(E),\n                 row_color = params$colsite, col_color = params$colspp,\n                 mult = mult,\n                 eig = lambda) +\n  ggtitle(\"WA scores for sites\")\n\n# LC scores\nglc &lt;- multiplot(indiv_row = Z2, indiv_col = Fhat, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS2, var_row_lab = colnames(E),\n                 row_color = params$colsite, col_color = params$colspp,\n                 mult = mult,\n                 eig = lambda) +\n  ggtitle(\"LC scores for sites\")\n\ngrid.arrange(grobs = list(gwa, glc),\n             nrow = 1)\n\n\n\n\n\n\n\nScaling type 3\nThis type of scaling is a compromise between scalings type 1 and 2.\n\nthe rows (sites) scores can be \\(\\hat{S}_3 = D(p_{i \\cdot})^{-1/2} \\hat{U} \\Lambda^{1/4}\\) or \\(Z_3 = D(p_{i\\cdot})^{-1/2} \\hat{\\bar{Q}} U \\Lambda^{-1/4}\\)\nthe columns (species) scores are \\(S_3 = D(p_{\\cdot j})^{-1/2} U \\Lambda^{1/4}\\)\nthe explanatory variable score are \\(BS_3 = E_{stand}' D(p_{i\\cdot})Z_{stand} \\Lambda^{1/4}\\)\n\n\n# Row scores\nShat3 &lt;- diag(dpi_^(-1/2)) %*% Uhat %*% Lambda^(1/4)\nZ3 &lt;- diag(dpi_^(-1/2)) %*% Qbarhat %*% U %*% diag(lambda^(-1/4))\n\n# Columns scores\nS3 &lt;- diag(dp_j^(-1/2)) %*% U %*% Lambda^(1/4)\n\n# Variables correlation\nBS3 &lt;- t(Estand) %*% Dpi_ %*% Zstand %*% Lambda^(1/4)\n\n\n\nCode\nmult &lt;- 5\n# WA scores\ngwa &lt;- multiplot(indiv_row = Shat3, indiv_col = S3, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS3, var_row_lab = colnames(E),\n                 row_color = params$colsite, col_color = params$colspp,\n                 mult = mult,\n                 eig = lambda) +\n  ggtitle(\"WA scores for sites\")\n\n# LC scores\nglc &lt;- multiplot(indiv_row = Z3, indiv_col = S3, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS3, var_row_lab = colnames(E),\n                 row_color = params$colsite, col_color = params$colspp,\n                 mult = mult,\n                 eig = lambda) +\n  ggtitle(\"LC scores for sites\")\n\ngrid.arrange(grobs = list(gwa, glc),\n             nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\hat{F}\\), \\(Z_1\\) and \\(BS_2\\) are the values used in the outputs of pcaiv.\n\ncca$co[, 1]/Fhat[, 1]\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\ncca$li[, 1]/Z1[, 1]\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\ncca$cor[, 1]/BS2[, 1]\n\n           location           elevation          patch_area        perc_forests \n                  1                   1                   1                   1 \n    perc_grasslands ShannonLandscapeDiv \n                  1                   1"
  },
  {
    "objectID": "CCA.html#interpretation",
    "href": "CCA.html#interpretation",
    "title": "Canonical Correspondence Analysis (CCA)",
    "section": "Interpretation",
    "text": "Interpretation\nIn CCA, each ordination axis corresponds to a linear combination of the explanatory variables that maximizes the explained variance in the response data.\nThis analysis constrains rows (sites) scores to be linear combinations of the environmental variables (scaling \\(Z_i\\)).\nSpecies can be ordered along environmental variables axes by projecting species coordinates on the vector of this variable. This gives species niche optimum, but there are strong assumptions:\n\nunimodal distribution of niches preferences along the variable of interest\nspecies distributions are indeed controlled by the environment\nthe study gradient is long enough to capture the range of species abundance variation.\n\nThe part of variation explained by the environmental variables can be computed as \\[\n\\frac{\\sum_k \\lambda_k(CCA)}{\\sum_l \\lambda_l(CA)}\n\\]\n\nsum(lambda)/sum(ca$eig)\n\n[1] 0.3795835\n\n\nHere, the environmental variables explain 38 % of the total variation.\nWarning: in CCA, using noisy or not relevant explanatory variables leads to spurious relationships.\n\nResidual analysis\nIn order to examinate residuals, a CA can be performed on the table of residuals \\(\\bar{Q}_{res}\\):\n\\[\n\\bar{Q}_{res} = \\bar{Q} - \\hat{\\bar{Q}}\n\\]\n\nQres &lt;- Qbar - Qbarhat\n\nres_pca &lt;- dudi.pca(Qres, scannf = FALSE, nf = 27)\n\nscatter(res_pca)\n\n\n\n\n\n\nTests of significance\nIt is possible to test the significance of the the relationship between \\(E\\) and \\(Y\\) with a permutation test.\n\nrandtest(cca, nrepet = 999)\n\nMonte-Carlo test\nCall: randtest.pcaiv(xtest = cca, nrepet = 999)\n\nObservation: 0.3795835 \n\nBased on 999 replicates\nSimulated p-value: 0.002 \nAlternative hypothesis: greater \n\n     Std.Obs  Expectation     Variance \n3.8530421764 0.2651328855 0.0008823253"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This website aims at summarizing the theory behind different methods to analyze count data:\nIt also describers reciprocal scaling (Thioulouse and Chessel 1992), which is a method to compute and visualize the dispersion among count tables analyzed with multivariate methods."
  },
  {
    "objectID": "index.html#dataset",
    "href": "index.html#dataset",
    "title": "Introduction",
    "section": "Dataset",
    "text": "Dataset\nThroughout this document, we will use the data from (Barbaro et al. 2012) on bird species distribution in New Zealand extracted from the CESTES database (Jeliazkov et al. 2020).\nThroughout this document, we will analyze a matrix \\(Y\\) (\\(r \\times c\\)) and two associated matrices \\(E\\) (\\(r \\times e\\)) and \\(T\\) (\\(t \\times c\\)). In our example:\n\n\\(Y\\) (\\(r =\\) 26 \\(\\times\\) \\(c =\\) 21) represents bird species counts in different sites\n\\(E\\) (\\(r =\\) 26 \\(\\times\\) \\(l =\\) 6) represents environmental variables in different sites\n\\(T\\) (\\(c =\\) 21 \\(\\times\\) \\(k =\\) 7) represents bird species traits"
  },
  {
    "objectID": "CA.html",
    "href": "CA.html",
    "title": "Correspondence analysis (CA)",
    "section": "",
    "text": "Code\n# Paths\nlibrary(here)\n\n# Multivariate analysis\nlibrary(ade4)\nlibrary(adegraphics)\n\n# Matrix algebra\nlibrary(expm)\n\n# Plots\nlibrary(gridExtra)\nsource(here(\"functions/plot.R\"))\nThe contents of this page relies heavily on (Legendre and Legendre 2012)."
  },
  {
    "objectID": "CA.html#introduction",
    "href": "CA.html#introduction",
    "title": "Correspondence analysis (CA)",
    "section": "Introduction",
    "text": "Introduction\nCorrespondence analysis (CA) was developed independently by several authors between the 1930s and 1960s. It has been mainly applied to analysis of ecological data tables (species \\(\\times\\) environment) such as the bat data we will analyze here.\nNotably, Hill discovered it in 1973 with an iterative method that he called reciprocal averaging. It involves ordering species with sites initially positioned randomly, reciprocally order sites with species, etc. This iterative procedure converges to reach the same results as what is achieved with CA’s first axis.\nCA is designed to analyze contingency tables (count tables) such \\(Y\\):\n\n\n\n\n\nHere is a preview of \\(Y\\) with our example dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsp1\nsp2\nsp3\nsp4\nsp5\nsp6\nsp7\nsp8\nsp9\nsp10\nsp11\nsp12\nsp13\nsp14\nsp15\nsp16\nsp17\nsp18\nsp19\nsp21\nsp22\n\n\n\n\n1\n0\n1\n1\n0\n1\n0\n2\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n0\n0\n2\n0\n1\n0\n5\n0\n2\n0\n2\n4\n0\n3\n3\n0\n3\n1\n0\n2\n\n\n5\n0\n0\n2\n0\n1\n0\n2\n0\n1\n2\n0\n0\n0\n0\n0\n0\n3\n2\n0\n1\n\n\n3\n0\n0\n0\n0\n1\n0\n4\n1\n0\n1\n0\n0\n0\n0\n0\n0\n2\n0\n0\n1\n\n\n5\n0\n0\n1\n0\n1\n0\n2\n0\n0\n3\n0\n0\n0\n0\n1\n0\n3\n1\n0\n1\n\n\n1\n0\n0\n1\n0\n2\n0\n3\n1\n2\n1\n0\n0\n0\n1\n0\n1\n2\n2\n0\n3\n\n\n4\n0\n0\n0\n0\n2\n0\n4\n0\n2\n0\n4\n0\n0\n0\n0\n1\n5\n0\n0\n2\n\n\n2\n0\n0\n1\n0\n2\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n5\n1\n5\n1\n\n\n4\n0\n0\n0\n0\n0\n0\n2\n0\n0\n3\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n\n\n2\n0\n1\n5\n0\n0\n0\n5\n0\n1\n1\n4\n0\n0\n0\n1\n2\n2\n0\n0\n2\n\n\n2\n0\n0\n2\n0\n1\n0\n5\n0\n2\n2\n0\n0\n0\n4\n0\n1\n10\n3\n0\n1\n\n\n4\n0\n0\n0\n0\n1\n0\n4\n0\n2\n2\n4\n0\n0\n0\n0\n2\n1\n2\n0\n1\n\n\n5\n5\n0\n0\n1\n1\n0\n1\n0\n3\n0\n5\n0\n0\n0\n0\n0\n0\n1\n5\n1\n\n\n4\n0\n0\n1\n0\n2\n0\n0\n0\n1\n2\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n\n\n3\n0\n0\n0\n1\n0\n0\n0\n0\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n5\n0\n4\n0\n1\n2\n0\n0\n0\n0\n0\n0\n1\n2\n0\n0\n\n\n4\n0\n1\n1\n0\n1\n0\n3\n1\n2\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n\n\n5\n0\n0\n1\n0\n1\n0\n3\n2\n1\n2\n0\n0\n0\n0\n0\n0\n1\n3\n0\n0\n\n\n4\n2\n0\n1\n0\n0\n0\n2\n0\n2\n2\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n4\n0\n0\n1\n0\n2\n0\n2\n2\n2\n1\n1\n0\n0\n0\n0\n0\n2\n0\n0\n0\n\n\n5\n0\n0\n1\n0\n4\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n\n\n5\n1\n0\n3\n0\n2\n0\n2\n1\n0\n2\n0\n0\n0\n0\n0\n0\n3\n1\n0\n0\n\n\n3\n5\n0\n4\n0\n2\n0\n5\n0\n4\n2\n0\n0\n1\n0\n0\n0\n2\n0\n0\n0\n\n\n5\n2\n0\n7\n0\n1\n1\n0\n0\n4\n2\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n\n\n6\n0\n0\n2\n0\n1\n0\n0\n0\n1\n3\n0\n0\n0\n0\n0\n0\n0\n3\n3\n0\n\n\n3\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n\n\n\n\n(r &lt;- dim(Y)[1])\n\n[1] 26\n\n(c &lt;- dim(Y)[2])\n\n[1] 21\n\n\nThis table represents the abundance of different bird species (columns) at different sites (rows) in New Zealand.\nMore generally CA, can be used to analyze tables which are dimensionnally homogeneous and contain no negative values.\nCA preserves \\(\\chi^2\\) distances \\(D\\) between rows and columns of the contingency table:\n\\[\nD(x_1, x_2) = \\sqrt{\\sum_{j = 1}^p \\frac{1}{y_{\\cdot j}/y_{\\cdot \\cdot}}\\left(\\frac{y_{1j}}{y_{1\\cdot}} -  \\frac{y_{2j}}{y_{2\\cdot}}\\right)^2}\n\\]\nWhere \\(x_1\\) and \\(x_2\\) represent two items of the rows/columns. \\(p\\) is the number of rows (if \\(x_1\\) and \\(x_2\\) are items of columns) or columns else.\n\\(y_{\\cdot j}\\) represent either:\n\nif \\(x_1\\) and \\(x_2\\) are items of the columns: the sum of the \\(j\\)-th row. For a species x environment table, it is the total number of individuals in the \\(j\\)-th site.\nif \\(x_1\\) and \\(x_2\\) are items of the rows: the sum of the \\(j\\)-th column. For a species x environment table, it is the total number of individuals of the \\(j\\)-th species.\n\n\\(y_{\\cdot \\cdot}\\) is the sum of all individuals of the table.\n\\(\\frac{y_{1j}}{y_{1\\cdot}}\\) are relative frequencies of individuals per rows (if \\(x_1\\) and \\(x_2\\) are items of the rows) or columns.\nFor example, we compute the \\(\\chi^2\\) distance between sp1 and sp2:\n\n# Get how much each site contributed to the total count of species\nfreq &lt;- apply(Y, 2, FUN = function(x) x/sum(x))\n\n# Get total number of individuals of bats in all sites\n(ytot &lt;- sum(Y))\n\n[1] 493\n\nsqrt( sum(1/(rowSums(Y)/ytot)*(freq[, \"sp1\"] - freq[, \"sp2\"])^2 ) )\n\n[1] 2.090546\n\n\nAnd between sites 1 and 2:\n\n# Get the relative composition of species for each site\nfreq &lt;- t(apply(Y, 1, FUN = function(x) x/sum(x)))\n\nsqrt( sum(1/(colSums(Y)/ytot)*(freq[\"1\",] - freq[\"2\", ])^2 ) )\n\n[1] 2.563363"
  },
  {
    "objectID": "CA.html#computation",
    "href": "CA.html#computation",
    "title": "Correspondence analysis (CA)",
    "section": "Computation",
    "text": "Computation\n\n\n\n\n\n\nTL;DR\n\n\n\nGiven a data matrix \\(Y\\), we “center-scale” this matrix (\\(\\bar{Q}\\)).\nWe can perform the SVD of \\(\\bar{Q}\\):\n\\[\n\\hat{Q} = \\hat{U} \\Delta U'\n\\]\n\\(U\\) (\\(c \\times c\\)) contains the loadings of the columns (species) of the contingency table.\n\\(\\hat{U}\\) (\\(r \\times c\\)) contains the loadings of the rows (sites) of the contingency table.\nThere are \\(\\min(c-1, r-1)\\) non-null eigenvalues. The CA eigenvalues are the squares of the SVD eigenvalues: \\(\\Lambda = \\Delta^2\\).\nAlso, there is a link between \\(\\hat{U}\\) and \\(U\\): \\[\n\\hat{U} = \\bar{Q}U\\Lambda^{-1/2}\n\\]\nand\n\\[\nU = \\bar{Q}'\\hat{U}\\Lambda^{-1/2}\n\\]\n\n\n\nTransform the table\nWe transform values of the contingency table \\(Y\\) with counts \\(f_{ij}\\) into proportions \\(p_{ij}\\):\n\\[\nP = Y/f_{\\cdot \\cdot}\n\\]\nWith our example data:\n\nP &lt;- Y/sum(Y)\n\nThen we transform values of this table into a quantity related to \\(\\chi^2\\) values \\(\\bar{Q}\\):\n\\[\n\\bar{Q} = [\\bar{q}_{ij}] = \\left[ \\frac{p_{ij} - p_{i\\cdot} p_{\\cdot j}}{\\sqrt{p_{i\\cdot} p_{\\cdot j}}} \\right]\n\\]\nThe \\(\\bar{q}_{ij}\\) values are equal to the \\(\\chi\\) values, a constant apart: \\(\\bar{q}_{ij} = \\chi_{ij}/\\sqrt{f_{\\cdot \\cdot}}\\).\nWith our data:\n\n# Initialize Qbar matrix\nQbar &lt;- matrix(ncol = ncol(Y), nrow = nrow(Y))\ncolnames(Qbar) &lt;- colnames(Y)\nrownames(Qbar) &lt;- rownames(Y)\n\nfor(i in 1:nrow(Y)) { # For each row\n  for (j in 1:ncol(Y)) { # For each column\n    # Do the sum\n    pi_ &lt;- sum(P[i, ])\n    p_j &lt;- sum(P[, j])\n    \n    # Compute the transformation\n    Qbar[i, j] &lt;- (P[i, j] - (pi_*p_j))/sqrt(pi_*p_j)\n  }\n}\n\nWe could also do this by matrix multiplication:\n\\[\n\\bar{Q} = D(p_{i\\cdot})^{-1/2} P D(p_{\\cdot j})^{-1/2}\n\\]\nIn (Carroll, Green, and Schaffer 1986), the formula is:\n\\[\n\\bar{Q} = D(y_{i\\cdot})^{-1/2} Y D(y_{\\cdot j})^{-1/2}\n\\]\nwhere matrices \\(D(p_{i \\cdot})\\) and \\(D(p_{\\cdot j})\\) are the diagonal matrices with the row and columns sums of \\(P\\) (respectively) and \\(D(y_{i \\cdot})\\) and \\(D(y_{\\cdot j})\\) are he equivalent for matrix \\(Y\\).\nNB: I could not verify this result numerically, since here \\(\\bar{Q}\\) is not equal to \\(D(p_{i \\cdot})^{-1/2} Q D(p_{\\cdot j})^{-1/2}\\), as in (Carroll, Green, and Schaffer 1987).\n\n# Define weights matrices\nDpi_ &lt;- diag(rowSums(P))\nDp_j &lt;- diag(colSums(P))\n\n# Also define a variable for just the sums for easier manipulation\ndpi_ &lt;- rowSums(P)\ndp_j &lt;- colSums(P)\n\n# Define weights matrices\nDyi_ &lt;- diag(rowSums(Y))\nDy_j &lt;- diag(colSums(Y))\n\n# Also define a variable for just the sums for easier manipulation\ndyi_ &lt;- rowSums(Y)\ndy_j &lt;- colSums(Y)\n\n\n\nSingular value decomposition of \\(\\bar{Q}\\)\n\\[\n\\bar{Q} = \\hat{U}WU'\n\\]\nWhere \\(\\bar{Q}\\) is \\(r \\times c\\), \\(\\hat{U}\\) is \\(r \\times c\\), \\(W\\) is a \\(c \\times c\\) diagonal matrix (assuming \\(r \\geq c\\); the table can be transposed to meet this condition) and \\(U'\\) is \\(c \\times c\\).\nNote that the last eigenvalue of this SVD will always be null (due to the centering).\n\nsv &lt;- svd(Qbar)\n\n# Eigenvalues\n(delta &lt;- sv$d) # Last eigenvalue is zero\n\n [1] 5.498025e-01 4.925288e-01 4.329553e-01 4.040318e-01 3.433964e-01\n [6] 3.150413e-01 2.883515e-01 2.863913e-01 2.408812e-01 2.195224e-01\n[11] 1.962294e-01 1.587138e-01 1.429179e-01 1.242514e-01 1.095957e-01\n[16] 9.878630e-02 7.702202e-02 6.088389e-02 5.470965e-02 4.087652e-02\n[21] 6.653482e-17\n\nlambda &lt;- delta^2\nLambda &lt;- diag(lambda)\n \n# Eigenvectors\nUhat &lt;- sv$u\nU &lt;- sv$v\n\nThis SVD is equivalent to a diagonalization: with the SVD, we can rewrite \\(\\bar{Q}' \\bar{Q}\\):\n\\[\n\\bar{Q}' \\bar{Q} = U W' \\hat{U'} \\hat{U} W U'\n\\]\nWe have \\(\\hat{U}' \\hat{U} = I\\) (because \\(\\hat{U}\\) is orthonormal), so:\n\\[\n\\bar{Q}' \\bar{Q} =  U W' W U'\n\\]\nIf we rewrite \\(W' W\\) as \\(\\Lambda\\) (so \\(W = \\Lambda^{1/2}\\)), and since \\(U' = U^{-1}\\) (\\(U\\) is orthonormal), then we can write this as a diagonalization:\n\\[\n\\bar{Q}' \\bar{Q} =  U \\Lambda U^{-1}\n\\]\nSimilarly, we can rewrite \\(\\bar{Q} \\bar{Q}'\\):\n\\[\n\\bar{Q} \\bar{Q}' =  \\hat{U} \\Lambda \\hat{U}^{-1}\n\\]\n\n# Diagonalize Q'Q ---\ndg1 &lt;- eigen(t(Qbar) %*% Qbar)\n\n# Eigenvalues are equal to the square roots of SVD eigenvalues\nall((dg1$values - lambda) &lt; 10e-10)\n\n[1] TRUE\n\n# Eigenvectors are equal to U\nall(abs(dg1$vectors/U) - 1 &lt; 10e-10)\n\n[1] TRUE\n\n\n\n# Diagonalize QQ' ---\ndg2 &lt;- eigen(Qbar %*% t(Qbar))\n\n# Eigenvalues are equal to the square roots of SVD eigenvalues\nall((dg1$values - lambda) &lt; 10e-10)\n\n[1] TRUE\n\n# Eigenvectors are equal to Uhat\nall(abs(dg2$vectors[, 1:(c-1)]/Uhat[, 1:(c-1)]) - 1 &lt; 10e-10)\n\n[1] TRUE"
  },
  {
    "objectID": "CA.html#scalings",
    "href": "CA.html#scalings",
    "title": "Correspondence analysis (CA)",
    "section": "Scalings",
    "text": "Scalings\n\n\n\n\n\n\nTL;DR\n\n\n\nTo plot rows and columns on separate plots, one can plot the eigenvectors \\(\\hat{U}\\) (rows = sites) and \\(U\\) (columns = species).\nTo plot columns and rows individuals on the same biplot, different scalings have been proposed. For sites (rows), these scalings are equal to \\(\\hat{S}_i = D(p_{i \\cdot})^{-1/2} \\hat{U} \\Delta^\\alpha\\) (with \\(\\alpha =\\) 1, 0 or 1/2).\nFor species (columns), these scalings are equal to \\(\\hat{S}_i = D(p_{\\cdot j})^{-1/2} U \\Delta^{1-\\alpha}\\).\n\nScaling type 1 (\\(\\alpha = 1\\)): this scaling preserves the distances between rows. Rows (sites) are represented with \\(F = D(p_{i \\cdot})^{-1/2} \\hat{U} \\Lambda^{1/2}\\) and species (columns) with \\(V = D(p_{\\cdot j})^{-1/2} U\\).\nScaling type 2 (\\(\\alpha = 0\\)): this scaling preserves distances between columns. Rows (sites) are represented with \\(\\hat{V} = D(p_{i \\cdot})^{-1/2} \\hat{U}\\) and species (columns) with \\(\\hat{F} = D(p_{\\cdot j})^{-1/2} U \\Lambda^{1/2}\\).\nScaling type 3 (\\(\\alpha = 1/2\\)): this scaling is a compromise between scalings type 1 and 2. Rows (sites) are represented with \\(\\hat{S}_3 = D(p_{i \\cdot})^{-1/2} \\hat{U} \\Lambda^{1/4}\\) and species (columns) with \\(S_3 = D(p_{\\cdot j})^{-1/2} U \\Lambda^{1/4}\\).\n\nAs noted in (Carroll, Green, and Schaffer 1987), if we plot distances of different scalings on the same biplot, we cannot interpret between-sets distances, but within-sets distances are still be interpretable. This is what the scaling type 4 described below does and this scaling is what (Carroll, Green, and Schaffer 1987) refers to.\n\n\nLet’s perform a CA with ade4 to check results later.\n\nca &lt;- dudi.coa(Y, \n               scannf = FALSE, \n               nf = min(r, c))\n\n\nNo scaling\nOne can represent the eigenvectors directly without scaling to display them on separate plots.\n\n\nCode\n# Plot sites ---\ng1 &lt;- multiplot(indiv_row = Uhat, \n          indiv_row_lab = rownames(Y), \n          row_color = params$colsite,\n          eig = lambda)\n\n# Plot species ---\ng2 &lt;- multiplot(indiv_row = U, \n          indiv_row_lab = colnames(Y), \n          row_color = params$colspp,\n          eig = lambda)\n\ngrid.arrange(grobs = list(g1, g2), nrow = 1)\n\n\n\n\n\n\n\nScaling type 1\nThis scaling preserves the row \\(\\chi^2\\) distances and is useful to interpret distances between row individuals (sites). In this scaling, the rows individuals are positioned at the barycentre of the corresponding column (species) individuals.\nFor that, we use:\n\n\\(F = D(p_{\\cdot j})^{-1/2} \\hat{U} \\Lambda^{1/2}\\) (rows = sites)\n\\(V = D(p_{\\cdot j})^{-1/2} U\\) (columns = species)\n\n\n# Sites\nF_ &lt;- diag(dpi_^(-1/2)) %*% Uhat %*% Lambda^(1/2)\n\n# Species\nV &lt;- diag(dp_j^(-1/2)) %*% U\n\nNB: \\(F\\) (sites scores) can also be computed from the species scores, since there is a correspondence between \\(U\\) and \\(\\hat{U}\\). This shows better the relationship between sites and species scores (sites are at the barycentre of their species with this scaling).\nNB2: I could not find back the formula in (Legendre and Legendre 2012) for \\(F = D(p_{i\\cdot})^{-1} Q V\\) and \\(\\hat{F} = D(p_{\\cdot j})^{-1} Q' \\hat{V}\\) (p 479). Indeed, for these formulas to be valid, we need \\(\\bar{Q} = D(p_{i \\cdot})^{-1/2} Q D(p_{\\cdot j})^{-1/2}\\), as in (Carroll, Green, and Schaffer 1987), but it is not the case here.\n\n# Compute sites coordinates using species coordinates\nF2 &lt;- diag(dpi_^(-1/2)) %*% Qbar %*% U\n\n# Check that this corresponds to F_ cmputed above\nall(F_[, 1:(c-1)]/F2[, 1:(c-1)] - 1 &lt; 10e-10)\n\n[1] TRUE\n\n\n\n\nCode\nmultiplot(indiv_row = F_, indiv_col = V, \n          indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n          row_color = params$colsite, col_color = params$colspp,\n          eig = lambda)\n\n\n\n\n\n\n\nScaling type 2\nThis preserves \\(\\chi^2\\) distances between species on the plot. Here, species are positioned at the centroid of sites.\nWe use:\n\n\\(\\hat{V} = D(p_{i\\cdot})^{-1/2} \\hat{U}\\) (rows = sites)\n\\(\\hat{F} = D(p_{\\cdot j})^{-1/2} U \\Lambda^{1/2}\\) (columns = species)\n\n\n# Sites\nVhat &lt;- diag(dpi_^(-1/2)) %*% Uhat\n\n# Species\nFhat &lt;- diag(dp_j^(-1/2)) %*% U %*% Lambda^(1/2)\n\nNB: like \\(F\\), \\(\\hat{F}\\) (species scores) can also be computed from the sites scores. This shows better the relationship between species and sites scores (species are at the barycentre of their sites with this scaling).\n\n# Compute sites coordinates using species coordinates\nFhat2 &lt;- diag(dp_j^(-1/2)) %*% t(Qbar) %*% Uhat\n\n# Check that this corresponds to Fhat cmputed above\nall(Fhat[, 1:(c-1)]/Fhat2[, 1:(c-1)] - 1 &lt; 10e-10)\n\n[1] TRUE\n\n\n\n\nCode\nmultiplot(indiv_row = Vhat, indiv_col = Fhat, \n          indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n          row_color = params$colsite, col_color = params$colspp,\n          eig = lambda)\n\n\n\n\n\n\n\nScaling type 3\nIt is a compromise between scalings 1 and 2. It preserves none of the \\(\\chi^2\\) distances.\nWe use:\n\n\\(\\hat{S}_3 = D(p_{i\\cdot})^{-1/2} \\hat{U} \\Lambda^{1/4}\\) (for sites)\n\\(S_3 = D(p_{\\cdot j})^{-1/2} U \\Lambda^{1/4}\\) (for species).\n\n\ns3_r &lt;- diag(dpi_^(-1/2)) %*% Uhat %*% Lambda^(1/4)\ns3_c &lt;- diag(dp_j^(-1/2)) %*% U %*% Lambda^(1/4)\n\n\n\nCode\nmultiplot(indiv_row = s3_r, indiv_col = s3_c, \n          indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n          row_color = params$colsite, col_color = params$colspp,\n          eig = lambda)\n\n\n\n\n\n\n\nScaling type 4\nUse \\(F\\) (sites) and \\(\\hat{F}\\) (species). Then, both \\(\\chi^2\\) distances are preserved. It is useful for a table crossing two factors.\n\n\nCode\nmultiplot(indiv_row = F_, indiv_col = Fhat, \n          indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n          row_color = params$colsite, col_color = params$colspp,\n          eig = lambda)"
  },
  {
    "objectID": "CA.html#interpretation",
    "href": "CA.html#interpretation",
    "title": "Correspondence analysis (CA)",
    "section": "Interpretation",
    "text": "Interpretation\nUsing the “transition formula” defined above (\\(\\hat{U} = \\bar{Q}U\\Lambda^{-1/2}\\)), we can relate \\(V\\) and \\(\\hat{V}\\) with the following equation:\n\\[\n\\hat{V} \\Lambda^{1/2} = D(p_{i\\cdot})^{-1/2}\\bar{Q}D(p_{\\cdot j})^{1/2}V\n\\]\nThis equation relates the ordination of rows (sites) (\\(\\hat{V}\\)) to the ordination of columns (species) (\\(V\\)). The “conversion factor” is \\(\\Lambda^{1/2}\\), which translates to \\(\\sqrt{\\lambda_h}\\) along principal axis \\(h\\).\nIndeed, the eigenvalue \\(\\lambda_h\\) is a measure of the correlation of the rows and columns ordinations along axis \\(h\\).\nIf species have a unimodal (bell-shaped) response along the gradient defined by the sites ordination along a given axis, then their position should be close to their niche optimum.\nDepending on he position of, say, species, in the multivariate space, there are several interpretations:\n\nspecies that are absent from most sites: often at the edge, close to a site where they happen to be present. They have little influence on the analysis.\nspecies that are in the center: either have their optimum there, they have a multimodal niche or their niche is not influenced by the latent variables of the ordination axes.\nspecies found away from the center but not at the edges: more likely to display clear relationships with the axis\n\n\n\n\n\n\n\nNote\n\n\n\nRare species are generally not very interesting because they are exceptions, not a general tendency. So some authors propose strategies to remove them, notably an iterative procedure. First do the complete CA, then remove the species seen one, twice… etc. Note the total inertia and the eigenvalues. When there is a jump, stop.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen there is a succession of species, along an hypothetical gradient, the CA may lead to an arch shape. There are techniques to detrend it, but knowing how to interpret this effect is probably more important."
  },
  {
    "objectID": "dcCA.html",
    "href": "dcCA.html",
    "title": "Double-constrained Correspondence Analysis (dc-CA)",
    "section": "",
    "text": "Code\n# Paths\nlibrary(here)\n\n# Multivariate analysis\nlibrary(ade4)\nlibrary(adegraphics)\n\n# dc-CA\nsource(here(\"functions/dpcaiv2-ade4.R\"))\n\n# Matrix algebra\nlibrary(expm)\n\n# Plots\nlibrary(gridExtra)\nsource(here(\"functions/plot.R\"))\nThe contents of this page relies heavily on (Braak, Šmilauer, and Dray 2018)."
  },
  {
    "objectID": "dcCA.html#introduction",
    "href": "dcCA.html#introduction",
    "title": "Double-constrained Correspondence Analysis (dc-CA)",
    "section": "Introduction",
    "text": "Introduction\ndc-CA was developed as a natural extension of CCA and has been used to study the relationship between species traits and environmental variables.\nIn dc-CA, we have 3 matrices:\n\nA data matrix \\(Y\\) (\\(r \\times c\\))\nA matrix of predictor variables \\(E\\) (\\(r \\times l\\))\nA matrix of predictor variables \\(T\\) (\\(c \\times k\\))\n\n\n\n\n\n\nThe aim of dc-CA is to find a linear combination of the predictor variables in \\(E\\) and \\(T\\) (environmental variables and traits) that maximizes the correlation.\nBelow are these matrices for our data:\n\\(Y =\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsp1\nsp2\nsp3\nsp4\nsp5\nsp6\nsp7\nsp8\nsp9\nsp10\nsp11\nsp12\nsp13\nsp14\nsp15\nsp16\nsp17\nsp18\nsp19\nsp21\nsp22\n\n\n\n\n1\n0\n1\n1\n0\n1\n0\n2\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n0\n0\n2\n0\n1\n0\n5\n0\n2\n0\n2\n4\n0\n3\n3\n0\n3\n1\n0\n2\n\n\n5\n0\n0\n2\n0\n1\n0\n2\n0\n1\n2\n0\n0\n0\n0\n0\n0\n3\n2\n0\n1\n\n\n3\n0\n0\n0\n0\n1\n0\n4\n1\n0\n1\n0\n0\n0\n0\n0\n0\n2\n0\n0\n1\n\n\n5\n0\n0\n1\n0\n1\n0\n2\n0\n0\n3\n0\n0\n0\n0\n1\n0\n3\n1\n0\n1\n\n\n1\n0\n0\n1\n0\n2\n0\n3\n1\n2\n1\n0\n0\n0\n1\n0\n1\n2\n2\n0\n3\n\n\n4\n0\n0\n0\n0\n2\n0\n4\n0\n2\n0\n4\n0\n0\n0\n0\n1\n5\n0\n0\n2\n\n\n2\n0\n0\n1\n0\n2\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n5\n1\n5\n1\n\n\n4\n0\n0\n0\n0\n0\n0\n2\n0\n0\n3\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n\n\n2\n0\n1\n5\n0\n0\n0\n5\n0\n1\n1\n4\n0\n0\n0\n1\n2\n2\n0\n0\n2\n\n\n2\n0\n0\n2\n0\n1\n0\n5\n0\n2\n2\n0\n0\n0\n4\n0\n1\n10\n3\n0\n1\n\n\n4\n0\n0\n0\n0\n1\n0\n4\n0\n2\n2\n4\n0\n0\n0\n0\n2\n1\n2\n0\n1\n\n\n5\n5\n0\n0\n1\n1\n0\n1\n0\n3\n0\n5\n0\n0\n0\n0\n0\n0\n1\n5\n1\n\n\n4\n0\n0\n1\n0\n2\n0\n0\n0\n1\n2\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n\n\n3\n0\n0\n0\n1\n0\n0\n0\n0\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n5\n0\n4\n0\n1\n2\n0\n0\n0\n0\n0\n0\n1\n2\n0\n0\n\n\n4\n0\n1\n1\n0\n1\n0\n3\n1\n2\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n\n\n5\n0\n0\n1\n0\n1\n0\n3\n2\n1\n2\n0\n0\n0\n0\n0\n0\n1\n3\n0\n0\n\n\n4\n2\n0\n1\n0\n0\n0\n2\n0\n2\n2\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n\n\n4\n0\n0\n1\n0\n2\n0\n2\n2\n2\n1\n1\n0\n0\n0\n0\n0\n2\n0\n0\n0\n\n\n5\n0\n0\n1\n0\n4\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n\n\n5\n1\n0\n3\n0\n2\n0\n2\n1\n0\n2\n0\n0\n0\n0\n0\n0\n3\n1\n0\n0\n\n\n3\n5\n0\n4\n0\n2\n0\n5\n0\n4\n2\n0\n0\n1\n0\n0\n0\n2\n0\n0\n0\n\n\n5\n2\n0\n7\n0\n1\n1\n0\n0\n4\n2\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n\n\n6\n0\n0\n2\n0\n1\n0\n0\n0\n1\n3\n0\n0\n0\n0\n0\n0\n0\n3\n3\n0\n\n\n3\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n\n\n\n\\(E =\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocation\nelevation\npatch_area\nperc_forests\nperc_grasslands\nShannonLandscapeDiv\n\n\n\n\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n0\n30\n7.92\n16.4129\n43.4066\n0.274\n\n\n0\n430\n83.24\n24.4526\n28.4995\n0.274\n\n\n0\n420\n140.83\n41.9966\n34.2412\n0.260\n\n\n0\n400\n140.83\n41.9966\n34.2412\n0.260\n\n\n0\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n0\n470\n7.84\n19.3087\n56.4031\n0.253\n\n\n0\n110\n21.15\n23.4668\n49.8908\n0.262\n\n\n0\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n0\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n0\n40\n6.66\n7.4729\n66.0061\n0.240\n\n\n0\n160\n5.63\n32.3516\n45.0477\n0.253\n\n\n0\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n1\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1\n30\n7.92\n16.4129\n43.4066\n0.274\n\n\n1\n430\n83.24\n24.4526\n28.4995\n0.274\n\n\n1\n420\n140.83\n41.9966\n34.2412\n0.260\n\n\n1\n400\n140.83\n41.9966\n34.2412\n0.260\n\n\n1\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n1\n470\n7.84\n19.3087\n56.4031\n0.253\n\n\n1\n110\n21.15\n23.4668\n49.8908\n0.262\n\n\n1\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n1\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n1\n40\n6.66\n7.4729\n66.0061\n0.240\n\n\n1\n160\n5.63\n32.3516\n45.0477\n0.253\n\n\n1\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n\n\n\n\\(T =\\)\n\n\n\n\n\n\nbiog\nforag\nmass\ndiet\nmove\nnest\neggs\n\n\n\n\nsp1\n1\n2\n2\n3\n1\n2\n2\n\n\nsp2\n1\n1\n1\n2\n1\n2\n2\n\n\nsp3\n1\n1\n2\n2\n2\n2\n1\n\n\nsp4\n1\n1\n1\n2\n1\n2\n2\n\n\nsp5\n1\n3\n3\n1\n2\n3\n4\n\n\nsp6\n1\n1\n4\n3\n2\n2\n1\n\n\nsp7\n1\n1\n1\n2\n1\n3\n3\n\n\nsp8\n1\n1\n1\n2\n2\n2\n2\n\n\nsp9\n1\n2\n1\n2\n1\n3\n3\n\n\nsp10\n1\n1\n1\n2\n1\n2\n2\n\n\nsp11\n2\n4\n3\n2\n1\n2\n2\n\n\nsp12\n2\n1\n2\n3\n2\n2\n3\n\n\nsp13\n2\n1\n4\n3\n2\n3\n1\n\n\nsp14\n2\n3\n2\n2\n1\n1\n3\n\n\nsp15\n2\n2\n2\n3\n2\n2\n3\n\n\nsp16\n2\n2\n2\n3\n2\n2\n4\n\n\nsp17\n2\n3\n4\n1\n1\n2\n2\n\n\nsp18\n2\n2\n1\n3\n2\n2\n3\n\n\nsp19\n2\n4\n3\n2\n1\n2\n2\n\n\nsp21\n2\n4\n3\n1\n2\n3\n3\n\n\nsp22\n2\n3\n2\n3\n2\n1\n3\n\n\n\n\n\n\n(r &lt;- dim(Y)[1])\n\n[1] 26\n\n(c &lt;- dim(Y)[2])\n\n[1] 21\n\n(l &lt;- dim(E)[2])\n\n[1] 6\n\n(k &lt;- dim(T_)[2])\n\n[1] 7\n\n\ndc-CA must not have to many traits compared to species: that is a disadvantage compared to RLQ, but on the other hand dc-CA allows to see relationships that RLQ would miss (Braak, Šmilauer, and Dray 2018).\nThere are several ways to perform dc-CA (Braak, Šmilauer, and Dray 2018), notably:\n\nsingular value decomposition (the method used here)\nan iterative method à la reciprocal averaging\ncanonical correlation analysis between \\(T\\) et \\(E\\), weighted by \\(Y\\)"
  },
  {
    "objectID": "dcCA.html#computation",
    "href": "dcCA.html#computation",
    "title": "Double-constrained Correspondence Analysis (dc-CA)",
    "section": "Computation",
    "text": "Computation\n\n\n\n\n\n\nTL;DR\n\n\n\nWe perform the SVD of \\(D = [E_{center}' D(y_{i \\cdot}) E_{center}]^{-1/2} E_{center}' Y T_{center} [T_{center}' D(y_{\\cdot j}) T_{center}]^{-1/2}\\)\n\\[\nD = P \\Delta Q'\n\\] This allows us to find the eigenvectors \\(P\\) (rows or environment eigenvectors) and \\(Q\\) (columns or traits eigenvectors).\nThe eigenvalues of the dc-CA are the squared eigenvalues of the SVD: \\(\\Lambda = \\Delta^2\\).\nThere are \\(\\min(k, l)\\) non-null eigenvalues???\nRegression coefficients\nThen, the regression coefficients for explanatory variables can be found with:\n\n\\(B = [E_{center}'D(y_{i\\cdot})E_{center}]^{-1/2}P\\Delta^\\alpha\\) (row explanatory variables = environmental variables)\n\\(C = [T_{center}' D(y_{\\cdot j})T_{center}]^{-1/2}Q\\Delta^\\alpha\\) (columns explanatory variables = species traits)\n\nwhere \\(\\alpha\\) is a scaling factor equal to 1, 0 or 1/2. The different scalings are detailed in the next part.\nIndividuals coordinates\nThe individuals coordinates (species or sites) can be computed in two ways:\nLinear combinations (LC scores) are computed from those coefficients :\n\n\\(Z_i = E_{center}B_i\\) for the rows (sites)\n\\(\\hat{Z}_i = T_{center}C_i\\) for columns (species)\n\nWeighted averages (WA scores) are computed from the scores of the other individuals:\n\n\\(\\hat{U}_i = D(y_{\\cdot i})^{-1} Y Z_i\\) for row (sites) scores\n\\(U_i = D(y_{j \\cdot})^{-1} Y' \\hat{Z}_i\\) for column (species) scores\n\nCorrelations\nFinally, we can compute correlations scores for different scalings for the explanatory variables:\n\n\\(BS_B^i = \\left[ E_{center}'D(y_{i\\cdot})E_{center} \\right]^{1/2} P \\Delta^{\\alpha}\\) for the row (environmental variables)\n\\(BS_C^i = \\left[ T_{center}'D(y_{i\\cdot})T_{center} \\right]^{1/2} Q \\Delta^{1-\\alpha}\\) for the columns (species traits)\n\n\n\nFirst, we need to center the traits and environment matrices (resp. \\(T_{cent}\\) and \\(E_{cent}\\)). To do that, we have to compute “inflated” versions of these matrices matching the occurrence counts in \\(Y\\).\n\\[\nE_{stand} = E - \\bar{E}_{infl} =\\left[e_i \\sum_i y_{i\\cdot}e_i/y_{\\cdot\\cdot} \\right]\n\\]\n\\[\nT_{stand} = T - \\bar{T}_{infl} = \\left[t_j \\sum_j y_{\\cdot j}t_j/y_{\\cdot\\cdot} \\right]\n\\]\nWith our data:\n\n# Center E -----\nyi_ &lt;- rowSums(Y)\nEcenter &lt;- matrix(nrow = nrow(E), ncol = ncol(E))\n\nfor(i in 1:ncol(Ecenter)) {\n  Ecenter[, i] &lt;- E[, i] - sum(E[, i]*yi_)/sum(Y)\n}\n# This is the same as computing a mean on inflated data matrix Einfl and centering E with these means\n\n# Center T -----\ny_j &lt;- colSums(Y)\nTcenter &lt;- matrix(nrow = nrow(T_), ncol = ncol(T_))\nrownames(Tcenter) &lt;- rownames(T_)\ncolnames(Tcenter) &lt;- colnames(T_)\n\nfor(j in 1:ncol(Tcenter)) {\n  Tcenter[, j] &lt;- T_[, j] - (sum(T_[, j]*y_j)/sum(Y))\n}\n\n\n# Check centering -----\nM1 &lt;- matrix(rep(1, nrow(Y)), nrow = 1)\nall((M1 %*% diag(rowSums(Y)) %*% Ecenter) &lt; 10e-10)\n\n[1] TRUE\n\nM1 &lt;- matrix(rep(1, ncol(Y)), nrow = 1)\nall((M1 %*% diag(colSums(Y)) %*% Tcenter) &lt; 10e-10)\n\n[1] TRUE\n\n\nMaximizing the fourth-corner correlation means finding \\(\\hat{u}\\) and \\(u\\) that maximize \\(\\hat{u}'Yu\\) (where \\(\\hat{u}\\) are the sites (rows) scores and \\(u\\) are the species (columns) scores).\n\\(\\hat{u}\\) and \\(u\\) are linear combinations of traits and environmental variables: \\(\\hat{u} = E_{center}b\\) and \\(u = T_{center}c\\).\nSo in the end, we need to maximize \\(\\hat{u}'Yu\\) with respect to the coefficients vectors \\(b\\) and \\(c\\):\n\\[\n\\max_{b, c}(\\hat{u}'Yu) = \\max_{b, c}\\left(\\left[E_{center}b\\right]'Y  T_{center}c \\right)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThese equations are written for the first axis, but we can also write them in matrix form:\n\\[\n\\max_{B, C}(\\hat{U}'YU) = \\max_{B, C}\\left(\\left[E_{center}B\\right]'Y T_{center}C \\right)\n\\]\n\n\nIn addition, we introduce the following constraint on the norm of the columns vectors of \\(\\hat{U}\\) and \\(U\\): \\(\\hat{u}' D(y_{i\\cdot}) \\hat{u} = 1\\) and \\(u' D(y_{\\cdot j})u = 1\\). In fact, tjese constraints will be relaxed later depending on the scaling (see below).\nTo find the coefficients \\(B\\) and \\(C\\) defined above, we need to diagonalize the following matrices \\(M\\) and \\(M_2\\):\n\nTo find \\(B\\), we must diagonalize \\(M\\): \\[\nM = \\left[E_{center}' D(y_{i\\cdot}) E_{center} \\right]^{-1} E_{center}'YT_{center} \\left[T_{center}' D(y_{\\cdot j}) T_{center} \\right]^{-1} T_{center}' Y' E_{center}\n\\] Where matrices \\(D(y_{i \\cdot})\\) and \\(D(y_{\\cdot j})\\) are the diagonal matrices with the column and row sums (respectively). They are analogous to matrices \\(D(p_{i \\cdot})\\) and \\(D(p_{\\cdot j})\\) defined for CA and CCA (but we use \\(Y\\) instead of \\(P\\) to define the margins).\n\nWe can view \\(M\\) as: \\[\nM = \\hat{E}_{center} \\hat{T}_{center} = \\beta T_{center} \\gamma E_{center}\n\\] With \\(\\beta = \\left[E_{center}' D(y_{i\\cdot}) E_{center} \\right]^{-1} E_{center}'Y\\) and \\(\\gamma = \\left[T_{center}' D(y_{\\cdot j}) T_{center} \\right]^{-1} T_{center}' Y'\\). Here, we predict the environment with traits so that \\(\\hat{E}_{center} = \\beta T_{center}\\) and the traits with the environment so that \\(\\hat{T}_{center} = \\gamma E_{center}\\). So we maximize the correlation between \\(\\hat{E}_{center}\\) and \\(\\hat{T}_{center}\\).\n\nSimilarly, to find \\(C\\), we must diagonalize \\(M_2\\):\n\n\\[\nM_2 = \\left[T_{center}' D(y_{\\cdot j}) T_{center} \\right]^{-1} T_{center}' Y' E_{center} \\left[E_{center}' D(y_{i\\cdot}) E_{center} \\right]^{-1} E_{center}'YT_{center}\n\\] Here, we can view \\(M_2\\) as: \\[\nM_2 = \\hat{T}_{center} \\hat{E}_{center} = \\gamma E_{center} \\beta T_{center}\n\\]\nThe eigenvectors matrices of these diagonalizations give us \\(B\\) and \\(C\\):\n\\[\nM = B \\Lambda_b B^{-1} ~~ \\text{and} ~~ M_2 = C \\Lambda_c C^{-1}\n\\]\nWe can either diagonalize \\(M\\) and \\(M_2\\) of perform a single SVD of a matrix \\(D\\) defined below and get a similar result. Both methods are presented below.\n\nDiagonalizations\nWe diagonalize \\(M\\) and \\(M_2\\):\n\n# Define weights\nDyi_ &lt;- diag(rowSums(Y))\nDy_j &lt;- diag(colSums(Y))\n\n# Compute M\nM &lt;- solve(t(Ecenter) %*% Dyi_ %*% Ecenter) %*% t(Ecenter) %*% Y %*% Tcenter %*% solve(t(Tcenter) %*% Dy_j %*% Tcenter) %*% t(Tcenter) %*% t(Y) %*% Ecenter\n\n# Compute M2\nM2 &lt;- solve(t(Tcenter) %*% Dy_j %*% Tcenter) %*% t(Tcenter) %*% t(Y) %*% Ecenter %*% solve(t(Ecenter) %*% Dyi_ %*% Ecenter) %*% t(Ecenter) %*% Y %*% Tcenter \n\n\n# Diagonalize M\neigB &lt;- eigen(M)\nlambdaB &lt;- eigB$values\nlambdaB # All non-null eigenvectors\n\n[1] 0.139487442 0.088736680 0.049183623 0.013797529 0.008736645 0.002485683\n\nvB &lt;- eigB$vectors\n\n# Diagonalize M2\neigC &lt;- eigen(M2)\nlambdaC &lt;- eigC$values\nlambdaC # six non-null eigenvalues\n\n[1]  1.394874e-01  8.873668e-02  4.918362e-02  1.379753e-02  8.736645e-03\n[6]  2.485683e-03 -9.364154e-18\n\nvC &lt;- eigC$vectors\n\nall(lambdaB - lambdaC[1:l] &lt; 10e-10)\n\n[1] TRUE\n\n\n\n\nSVD\nAlternatively, we can compute the SVD of a matrix \\(D\\) computed from \\(M\\) and get a similar result. \\(D\\) is defined as:\n\\[\nD = M^{1/2} = [E_{center}' D(y_{i \\cdot}) E_{center}]^{-1/2} E_{center}' Y T_{center} [T_{center}' D(y_{\\cdot j}) T_{center}]^{-1/2}\n\\]\nWith our dataset:\n\nD &lt;- solve(sqrtm(t(Ecenter) %*% Dyi_ %*% Ecenter)) %*%\n  t(Ecenter) %*% Y %*% Tcenter %*%\n  solve(sqrtm(t(Tcenter) %*% Dy_j %*% Tcenter))\n\nWe perform the SVD of \\(D\\): \\(D = P \\Lambda_{SVD} Q'\\).\n\nsv &lt;- svd(D)\n\ndelta &lt;- sv$d\nDelta &lt;- diag(delta)\n\nP_svd &lt;- sv$u\nQ_svd &lt;- sv$v\n\ndim(P_svd)\n\n[1] 6 6\n\ndim(Q_svd)\n\n[1] 7 6\n\n\nThe eigenvalues of the SVD are the square roots of the final eigenvalues obtained by diagonalization: \\(\\Lambda = \\Delta^2\\)\n\nlambda &lt;- delta^2\nLambda &lt;- diag(lambda)"
  },
  {
    "objectID": "dcCA.html#scalings",
    "href": "dcCA.html#scalings",
    "title": "Double-constrained Correspondence Analysis (dc-CA)",
    "section": "Scalings",
    "text": "Scalings\n\n\n\n\n\n\nTL;DR\n\n\n\nThere are two types of coordinates: linear combination scores (LC scores) and weighted averages scores (WA scores).\nThe general formulas are:\n\nthe regression coefficients for the rows (environmental variables) are \\(B = [E_{center}'D(y_{i\\cdot})E_{center}]^{-1/2}P\\Delta^\\alpha\\)\nthe regression coefficients for the columns (species traits) are \\(C = [T_{center}' D(y_{\\cdot j})T_{center}]^{-1/2}Q\\Delta^{1 - \\alpha}\\)\nthe LC scores for rows (sites) are \\(Z_i = E_{center}B_i\\)\nthe LC scores for columns (species) are \\(\\hat{Z}_i = T_{center}C_i\\)\nthe WA scores for rows (sites) are \\(\\hat{U}_i = D(y_{\\cdot i})^{-1} Y Z_i\\)\nthe WA scores for columns (species) are \\(U_i = D(y_{j \\cdot})^{-1} Y' \\hat{Z}_i\\)\nthe correlations with axes for the rows (environmental variables) are \\(BS_B^i = \\left[ E_{center}'D(y_{i\\cdot})E_{center} \\right]^{1/2} P \\Delta^{\\alpha}\\)\nthe correlations with axes for the columns (species traits) are \\(BS_C^i = \\left[ T_{center}'D(y_{i\\cdot})T_{center} \\right]^{1/2} Q \\Delta^{1-\\alpha}\\)\n\nNote: the formula for the regression coefficients \\(C_i\\) are in disagreement with the article of (Braak, Šmilauer, and Dray 2018). Here, we find that the scaling should include \\(\\Delta^{1-\\alpha}\\) in the formula (17) instead of \\(\\Delta^{\\alpha-1}\\). However, when we try to prove transition formula (6), we find that it should be \\(\\Delta^{\\alpha}\\).\nIn these formulas, note that the WA scores for one dimension are computed from the predicted scores of the other dimension.\nScaling type 1 (\\(\\alpha = 1\\))\n\nthe regression coefficients for the rows (environmental variables) are \\(B_1 = [E_{center}'D(y_{i\\cdot})E_{center}]^{-1/2}P\\Delta\\)\nthe regression coefficients for the columns (species traits) are \\(C_1 = [T_{center}' D(y_{\\cdot j})T_{center}]^{-1/2}Q\\)\nthe LC scores for rows (sites) are \\(Z_1 = E_{center}B_1\\)\nthe LC scores for columns (species) are \\(\\hat{Z}_1 = T_{center}C_1\\)\nthe WA scores for rows (sites) are \\(\\hat{U}_1 = D(y_{\\cdot i})^{-1} Y Z_1\\)\nthe WA scores for columns (species) are \\(U_1 = D(y_{j \\cdot})^{-1} Y' \\hat{Z}_1\\)\nthe correlations with axes for the rows (environmental variables) are \\(BS_B^1 = \\left[ E_{center}'D(y_{i\\cdot})E_{center} \\right]^{1/2} P \\Delta\\)\nthe correlations with axes for the columns (species traits) are \\(BS_C^1 = \\left[ T_{center}'D(y_{i\\cdot})T_{center} \\right]^{1/2} Q \\Delta\\)\n\nScaling type 2 (\\(\\alpha = 0\\))\n\nthe regression coefficients for the rows (environmental variables) are \\(B_2 = [E_{center}'D(y_{i\\cdot})E_{center}]^{-1/2}P\\)\nthe regression coefficients for the columns (species traits) are \\(C_2 = [T_{center}' D(y_{\\cdot j})T_{center}]^{-1/2}Q \\Delta\\)\nthe LC scores for rows (sites) are \\(Z_2 = E_{center}B_2\\)\nthe LC scores for columns (species) are \\(\\hat{Z}_2 = T_{center}C_2\\)\nthe WA scores for rows (sites) are \\(\\hat{U}_2 = D(y_{\\cdot i})^{-1} Y Z_2\\)\nthe WA scores for columns (species) are \\(U_2 = D(y_{j \\cdot})^{-1} Y' \\hat{Z}_2\\)\nthe correlations with axes for the rows (environmental variables) are \\(BS_B^2 = \\left[ E_{center}'D(y_{i\\cdot})E_{center} \\right]^{1/2} P\\)\nthe correlations with axes for the columns (species traits) are \\(BS_C^2 = \\left[ T_{center}'D(y_{i\\cdot})T_{center} \\right]^{1/2} Q\\)\n\nScaling type 3 (\\(\\alpha = 1/2\\))\n\nthe regression coefficients for the rows (environmental variables) are \\(B_3 = [E_{center}'D(y_{i\\cdot})E_{center}]^{-1/2}P\\Delta^{1/2}\\)\nthe regression coefficients for the columns (species traits) are \\(C_3 = [T_{center}' D(y_{\\cdot j})T_{center}]^{-1/2}Q\\Delta^{1/2}\\)\nthe LC scores for rows (sites) are \\(Z_3 = E_{center}B_3\\)\nthe LC scores for columns (species) are \\(\\hat{Z}_3 = T_{center}C_3\\)\nthe WA scores for rows (sites) are \\(\\hat{U}_3 = D(y_{\\cdot i})^{-1} Y Z_3\\)\nthe WA scores for columns (species) are \\(U_3 = D(y_{j \\cdot})^{-1} Y' \\hat{Z}_3\\)\nthe correlations with axes for the rows (environmental variables) are \\(BS_B^3 = \\left[ E_{center}'D(y_{i\\cdot})E_{center} \\right]^{1/2} P \\Delta^{1/2}\\)\nthe correlations with axes for the columns (species traits) are \\(BS_C^3 = \\left[ T_{center}'D(y_{i\\cdot})T_{center} \\right]^{1/2} Q \\Delta^{1/2}\\)\n\n\\(\\alpha\\) changes the interpretation of the correlations vectors:\n\nintra-set correlations: \\(BS_B^2\\) (\\(\\alpha = 0\\)) approximates the correlations between the environmental variables (rows variables) and \\(BS_C^1\\) (\\(\\alpha = 1\\)) approximates the correlations between the traits (columns variables)\ninter-set correlations (fourth-corner): \\(BS_B^1\\) (\\(\\alpha = 1\\)) approximates the correlation between environmental variables (row variables) and species (columns) and \\(BS_C^2\\) (\\(\\alpha = 0\\)) approximates the correlation between traits (columns variables) and sites (rows)\n\\(BS_B^3\\) and \\(BS_C^3\\) (\\(\\alpha = 1/2\\)) is the geometric mean of scalings 1 and 2.\n\nWhen plotting the correlation circle, to look at the correlations between variables of the same set (traits or environmental variables), we should use \\(BS_B^2\\) and \\(BS_C^1\\). There are the scores returned by ade4.\n\n\n\nca &lt;- dudi.coa(Y, \n               nf = c-1, \n               scannf = FALSE)\n\ndcca &lt;- dpcaiv2(dudi = ca, \n                dfR = E,\n                dfQ = T_,\n                scannf = FALSE, \n                nf = min(k, l))\n\n\nPre-computations\nCoefficients etc computed with our method are equal to those computed by ade4, but we need to multiply them by a scaling factor \\(\\sqrt{y_{\\cdot \\cdot}}\\).\n\n(scaling &lt;- sqrt(sum(Y)))\n\n[1] 22.2036\n\n\nWe will need this function to normalize correlations vectors to compare results to those of ade4.\n\n#' Normalize row or columns vectors of a matrix\n#'\n#' @param M the matrix to normalize\n#' @param margin the margin (1 = rows, 2 = columns)\n#'\n#' @return The normalized matrix M\nnormalize &lt;- function(M, margin) {\n  \n  m_norm &lt;- apply(M,\n                  margin, \n                  function(x) sqrt(sum(x^2)))\n  M_norm &lt;- sweep(M, margin, m_norm, \"/\")\n  return(M_norm)\n}\n\n\n\nScaling type 1\nThis type of scaling preserves the distances between rows.\n\nthe rows (sites) scores can be \\(\\hat{U}_1\\) (WA scores) or \\(\\hat{Z}_1\\) (LC scores)\n\nthe columns (species) scores can be \\(U_1\\) (WA scores) or \\(Z_1\\) (LC scores)\nrows variables (environmental variables) correlations are \\(BS_{B1}\\).\ncolumns variables (species traits) correlations are \\(BS_{C1}\\)\n\nWith the scaling type 1, \\(BS_{B1}\\) represents the correlation between environmental variables and species and \\(BS_{C1}\\) represents the correlation between species traits and species.\n\n# Coefficients\nB_1 &lt;- solve(sqrtm(t(Ecenter) %*% Dyi_ %*% Ecenter)) %*%  P_svd %*% Delta\nC_1 &lt;- solve(sqrtm(t(Tcenter) %*% Dy_j %*% Tcenter)) %*% Q_svd\n\n# LC scores\nZhat1 &lt;- Ecenter %*% B_1 # rows\nZ1 &lt;- Tcenter %*% C_1 # columns\n\n# WA scores\nUhat1 &lt;- solve(Dyi_) %*% Y %*% Z1 # rows\nU1 &lt;- solve(Dy_j) %*% t(Y) %*% Zhat1  # columns\n\n# Variables scores\nBS_B1 &lt;- sqrtm(t(Ecenter) %*% Dyi_ %*% Ecenter) %*% P_svd %*% Delta\nBS_C1 &lt;- sqrtm(t(Tcenter) %*% Dy_j %*% Tcenter) %*% Q_svd\n\n# Normalize\nBS_B1norm &lt;- normalize(BS_B1, 1)\nBS_C1norm &lt;- normalize(BS_C1, 1)\n\n\n\nCode\n# WA scores\ngwa &lt;- multiplot(indiv_row = Uhat1*scaling, indiv_col = U1*scaling, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS_B1norm, var_row_lab = colnames(E),\n                 var_col = BS_C1norm, var_col_lab = colnames(T_),\n                 row_color = params$colsite, col_color = params$colspp,\n                 eig = lambda) +\n  ggtitle(\"WA scores for sites\")\n\n# LC scores\nglc &lt;- multiplot(indiv_row = Zhat1*scaling, indiv_col = Z1*scaling, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS_B1norm, var_row_lab = colnames(E),\n                 var_col = BS_C1norm, var_col_lab = colnames(T_),\n                 row_color = params$colsite, col_color = params$colspp,\n                 eig = lambda) +\n  ggtitle(\"LC scores for sites\")\n\ngrid.arrange(grobs = list(gwa, glc),\n             nrow = 2)\n\n\n\n\n\nOn this plot:\n\nrow and columns variables (\\(BS_B^1\\) - \\(BS_C^1\\)): angles between arrows of rows and environmental variables represent their fourth-corner correlation. For instance, the correlation between mass and perc_forests is large (arrows size + direction).\nspecies traits (\\(BS_C^1\\)): arrows indicate intra-set correlations.\nspecies and environmental variables (\\(U\\) - \\(BS_B^1\\)): species points on the right are located in sites with a high location variable.\nsites and species traits (\\(\\hat{U}\\) - \\(BS_C^1\\)): we can understand sites using their community weighted mean (eg site 26 has birds with high mass).\nspecies and species traits (\\(U\\) - \\(BS_C^1\\)): for instance, species 12 has large eggs.\n\nThere is no interpretation for sites - environmental variables (\\(\\hat{U}\\) - \\(BS_B^1\\)).\n\n\nScaling type 2\nThis type of scaling preserves the distances between columns.\n\nthe rows (sites) scores can be \\(\\hat{U}_2\\) (WA scores) or \\(\\hat{Z}_2\\) (LC scores)\n\nthe columns (species) scores can be \\(U_2\\) (WA scores) or \\(Z_2\\) (LC scores)\nrows variables (environmental variables) correlations are \\(BS_{B2}\\).\ncolumns variables (species traits) correlations are \\(BS_{C2}\\)\n\nWith the scaling type 2, \\(BS_{B2}\\) represents the correlation between environmental variables and sites and \\(BS_{C2}\\) represents the correlation between species traits and sites.\n\n# Regression coefficients\nB_2 &lt;- solve(sqrtm(t(Ecenter) %*% Dyi_ %*% Ecenter)) %*%  P_svd\nC_2 &lt;- solve(sqrtm(t(Tcenter) %*% Dy_j %*% Tcenter)) %*% Q_svd %*% Delta\n\n# LC scores\nZhat2 &lt;- Ecenter %*% B_2 # rows\nZ2 &lt;- Tcenter %*% C_2 # columns\n\n# WA scores\nUhat2 &lt;- solve(Dyi_) %*% Y %*% Z2 # rows\nU2 &lt;- solve(Dy_j) %*% t(Y) %*% Zhat2 # columns\n\n# Variables scores\nBS_B2 &lt;- sqrtm(t(Ecenter) %*% Dyi_ %*% Ecenter) %*% P_svd\nBS_C2 &lt;- sqrtm(t(Tcenter) %*% Dy_j %*% Tcenter) %*% Q_svd %*% Delta\n\n# Normalize\nBS_B2norm &lt;- normalize(BS_B2, 1)\nBS_C2norm &lt;- normalize(BS_C2, 1)\n\n\n\nCode\n# WA scores\ngwa &lt;- multiplot(indiv_row = Uhat2*scaling, indiv_col = U2*scaling, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS_B2norm, var_row_lab = colnames(E),\n                 var_col = BS_C2norm, var_col_lab = colnames(T_),\n                 row_color = params$colsite, col_color = params$colspp,\n                 eig = lambda) +\n  ggtitle(\"WA scores for sites\")\n\n# LC scores\nglc &lt;- multiplot(indiv_row = Zhat2*scaling, indiv_col = Z2*scaling, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS_B2norm, var_row_lab = colnames(E),\n                 var_col = BS_C2norm, var_col_lab = colnames(T_),\n                 row_color = params$colsite, col_color = params$colspp,\n                 eig = lambda) +\n  ggtitle(\"LC scores for sites\")\n\ngrid.arrange(grobs = list(gwa, glc),\n             nrow = 2)\n\n\n\n\n\nOn this plot:\n\nrow and columns variables (\\(BS_B^2\\) - \\(BS_C^2\\)): angles between arrows of rows and environmental variables represent their fourth-corner correlation. For instance, the correlation between mass and patch_forests is large (arrows size + direction).\nenvironmental variables (\\(BS_B^2\\)): arrows indicate intra-set correlations.\nspecies and environmental variables (\\(U\\) - \\(BS_B^1\\))\nsites and species traits (\\(\\hat{U}\\) - \\(BS_C^1\\))\nsites and environmental variables (\\(U\\) - \\(BS_C^1\\))\n\nThere is no interpretation for species - species traits (\\(U\\) - \\(BS_C^2\\)).\n\n\nScaling type 3\nThis type of scaling is an intermediate between scalings 1 and 2.\n\nthe rows (sites) scores can be \\(\\hat{U}_3\\) (WA scores) or \\(\\hat{Z}_3\\) (LC scores)\n\nthe columns (species) scores can be \\(U_3\\) (WA scores) or \\(Z_3\\) (LC scores)\nrows variables (environmental variables) correlations are \\(BS_{B3}\\).\ncolumns variables (species traits) correlations are \\(BS_{C3}\\)\n\nWith the scaling type 3, \\(BS_{B3}\\) and \\(BS_{C3}\\) represent the geometric mean of their correlation with species and sites.\n\n# Variables coefficients\nB_3 &lt;- solve(sqrtm(t(Ecenter) %*% Dyi_ %*% Ecenter)) %*%  P_svd %*% Delta^(1/2)\nC_3 &lt;- solve(sqrtm(t(Tcenter) %*% Dy_j %*% Tcenter)) %*% Q_svd %*% Delta^(1/2)\n\n# LC scores\nZhat3 &lt;- Ecenter %*% B_3 # rows\nZ3 &lt;- Tcenter %*% C_3 # columns\n\n# WA scores\nUhat3 &lt;- solve(Dyi_) %*% Y %*% Tcenter %*% C_3 # rows\nU3 &lt;- solve(Dy_j) %*% t(Y) %*% Zhat3 # columns\n\n# Variables scores\nBS_B3 &lt;- sqrtm(t(Ecenter) %*% Dyi_ %*% Ecenter) %*% P_svd %*% Delta^(1/2)\nBS_C3 &lt;- sqrtm(t(Tcenter) %*% Dy_j %*% Tcenter) %*% Q_svd %*% Delta^(1/2)\n\n# Normalize\nBS_B3norm &lt;- normalize(BS_B3, 1)\nBS_C3norm &lt;- normalize(BS_C3, 1)\n\n\n\nCode\n# WA scores\ngwa &lt;- multiplot(indiv_row = Uhat3*scaling, indiv_col = U3*scaling, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS_B3norm, var_row_lab = colnames(E),\n                 var_col = BS_C3norm, var_col_lab = colnames(T_),\n                 row_color = params$colsite, col_color = params$colspp,\n                 eig = lambda) +\n  ggtitle(\"WA scores for sites\")\n\n# LC scores\nglc &lt;- multiplot(indiv_row = Zhat3*scaling, indiv_col = Z3*scaling, \n                 indiv_row_lab = rownames(Y), indiv_col_lab = colnames(Y), \n                 var_row = BS_B3norm, var_row_lab = colnames(E),\n                 var_col = BS_C3norm, var_col_lab = colnames(T_),\n                 row_color = params$colsite, col_color = params$colspp,\n                 eig = lambda) +\n  ggtitle(\"LC scores for sites\")\n\ngrid.arrange(grobs = list(gwa, glc),\n             nrow = 2)"
  },
  {
    "objectID": "dcCA.html#more-tests",
    "href": "dcCA.html#more-tests",
    "title": "Double-constrained Correspondence Analysis (dc-CA)",
    "section": "More tests",
    "text": "More tests\nBelow, we test the variances of various scores.\n\n# Since ter Braak and ade4 disagree, check the variance of co and Z2 or Z2 (it should be the eigenvalues of each axis)\n\n#' Get the variance of vector x\n#'\n#' @param x The vector\n#' @param w The weights\n#'\n#' @return The variance\nvarvec &lt;- function(x, w) {\n  sum(w*(x-mean(x))^2)\n}\n\nres_ade4 &lt;- apply(as.matrix(dcca$co), \n                  2, \n                  function(x) varvec(x, dcca$cw))\nres_ade4/dcca$eig\n\n   Comp1    Comp2    Comp3    Comp4    Comp5    Comp6 \n1.106286 1.151797 1.466209 1.038659 1.005454 1.000003 \n\nres_tB &lt;- apply(Z2*scaling, \n                2, \n                function(x) varvec(x, dcca$cw))\nres_tB/dcca$eig \n\n[1] 1.106286 1.151797 1.466209 1.038659 1.005454 1.000003\n\nres_agree &lt;- apply(Z2*scaling, \n                   2, \n                   function(x) varvec(x, dcca$cw))\nres_agree/dcca$eig\n\n[1] 1.106286 1.151797 1.466209 1.038659 1.005454 1.000003\n\n\n\nLC scores for rows:\n\n\n# l1 variances should be 1\napply(as.matrix(dcca$l1), \n      2, \n      function(x) varvec(x, dcca$lw))\n\n     RS1      RS2      RS3      RS4      RS5      RS6 \n1.014490 1.005249 1.002007 1.000012 1.000331 1.000000 \n\n# li variances should be eigenvalues\napply(as.matrix(dcca$li), \n      2, \n      function(x) varvec(x, dcca$lw))/dcca$eig\n\n   Axis1    Axis2    Axis3    Axis4    Axis5    Axis6 \n1.014490 1.005249 1.002007 1.000012 1.000331 1.000000 \n\n\n\nLC scores for columns\n\n\n# c1 variances should be 1\napply(as.matrix(dcca$c1), \n      2, \n      function(x) varvec(x, dcca$cw))\n\n     CS1      CS2      CS3      CS4      CS5      CS6 \n1.106286 1.151797 1.466209 1.038659 1.005454 1.000003 \n\n# co variances should be eigenvalues\napply(as.matrix(dcca$co), \n      2, \n      function(x) varvec(x, dcca$cw))/dcca$eig \n\n   Comp1    Comp2    Comp3    Comp4    Comp5    Comp6 \n1.106286 1.151797 1.466209 1.038659 1.005454 1.000003 \n\n\n\n# Z2 variances should be the eigenvalues\napply(Z2*scaling, \n      2, \n      function(x) varvec(x, dcca$cw))/dcca$eig\n\n[1] 1.106286 1.151797 1.466209 1.038659 1.005454 1.000003\n\n\n\nWA scores:\n\n\n# Hypothesis: these scires shound be the eigenvalues\napply(as.matrix(dcca$lsR), \n      2, \n      function(x) varvec(x, dcca$lw))/dcca$eig\n\n    Axis1     Axis2     Axis3     Axis4     Axis5     Axis6 \n 1.397739  1.490618  1.894509  3.367612  7.569186 16.600310 \n\n# co variances should be eigenvalues\napply(as.matrix(dcca$lsQ), \n      2, \n      function(x) varvec(x, dcca$cw))/dcca$eig\n\n    Comp1     Comp2     Comp3     Comp4     Comp5     Comp6 \n 1.356947  1.416201  2.521861  7.980247  5.161038 13.632366"
  },
  {
    "objectID": "dcCA.html#interpretation",
    "href": "dcCA.html#interpretation",
    "title": "Double-constrained Correspondence Analysis (dc-CA)",
    "section": "Interpretation",
    "text": "Interpretation\nThis method finds the linear correlation of row explanatory variables (environmental variables) and the linear correlation of columns explanatory variables (species traits) that maximizes the fourth-corner correlation, i.e. the correlation between these linear combinations of row and columns-variables.\nThere are other related methods, that have been better described and also more used in ecology: RLQ, community weighted means RDA (CMW-RDA).\nContrary to RLQ, dc-CA takes into account the correlation between the row and column variables. Thus, while RLQ can analyze any number of row and column variables, it is not the case with dc-CA the number of row and column variables must not be large compared to the number of rows/columns in the tables. Also, CCA maximizes correlation and RLQ maximizes covariance (Braak, Šmilauer, and Dray 2018).\nThe eigenvalues of dc-CA are the squares of the fourth-corner correlations."
  },
  {
    "objectID": "recscal_cca.html",
    "href": "recscal_cca.html",
    "title": "Reciprocal scaling with CCA",
    "section": "",
    "text": "Code\n# Paths\nlibrary(here)\n\n# Multivariate analysis\nlibrary(ade4)\nlibrary(adegraphics)\n\n# dc-CA\nsource(here(\"functions/dpcaiv2-ade4.R\"))\n\n# Reciprocal scaling\nsource(here(\"functions/reciprocal.R\"))\n\n# Matrix algebra\nlibrary(expm)\n\n# Plots\nsource(here(\"functions/plot.R\"))\nlibrary(gridExtra)"
  },
  {
    "objectID": "recscal_cca.html#introduction",
    "href": "recscal_cca.html#introduction",
    "title": "Reciprocal scaling with CCA",
    "section": "Introduction",
    "text": "Introduction\nThis is an extension of reciprocal scaling defined for correspondence analysis by Thioulouse and Chessel (1992) to canonical correspondence analysis.\nHere, we start from two matrices:\n\nA data matrix \\(Y\\) (\\(r \\times c\\))\nA matrix of predictor variables (environmental variabnes) \\(E\\) (\\(r \\times l\\))\n\n\n\n\n\n\n\n(r &lt;- dim(Y)[1])\n\n[1] 26\n\n(c &lt;- dim(Y)[2])\n\n[1] 21\n\n(l &lt;- ncol(E))\n\n[1] 6"
  },
  {
    "objectID": "recscal_cca.html#computation",
    "href": "recscal_cca.html#computation",
    "title": "Reciprocal scaling with CCA",
    "section": "Computation",
    "text": "Computation\n\nFrom CCA scores\nWe compute the \\(H^{CCA}_k(i, j)\\) from the WA (for columns = species) and LC (for rows = sites) scores computed with CCA (noted \\(L^{CCA}\\) and \\(C^{CCA}\\)). This formula is a direct extension of formula (11) in Thioulouse and Chessel (1992) but we replace the CA ordination scores with the CCA ordination scores.\n\\[\nH_k^{CCA}(i, j) = \\frac{L^{CCA}_k(i) + C^{CCA}_k(j)}{\\sqrt{2 \\lambda_k \\mu_k}}\n\\]\n\nYdf &lt;- as.data.frame(Y)\nca &lt;- dudi.coa(Ydf, \n               scannf = FALSE,\n               nf = min(r - 1, c - 1))\n\nneig &lt;- min(c(r-1, c, l))\ncca &lt;- pcaiv(dudi = ca, \n             df = E,\n             scannf = FALSE,\n             nf = neig)\n\nL_CCA &lt;- cca$li\nC_CCA &lt;- cca$co\n\nlambda_CCA &lt;- cca$eig\nmu_CCA &lt;- 1 + sqrt(lambda_CCA)\n\nWe also compute reciprocal scaling for comparison:\n\nrec_cca &lt;- reciprocal.caiv(cca)\n\n\n# Transform matrix to count table\nYfreq &lt;- as.data.frame(as.table(Y))\ncolnames(Yfreq) &lt;- c(\"row\", \"col\", \"Freq\")\n\n# Remove the cells with no observation\nYfreq0 &lt;- Yfreq[-which(Yfreq$Freq == 0),]\nYfreq0$colind &lt;- match(Yfreq0$col, colnames(Y)) # match index and species names\n\n\n# Initialize results matrix\nH_CCA &lt;- matrix(nrow = nrow(Yfreq0), \n                ncol = length(lambda_CCA))\n\nfor (k in 1:length(lambda_CCA)) { # For each axis\n  ind &lt;- 1 # initialize row index\n  for (obs in 1:nrow(Yfreq0)) { # For each observation\n    i &lt;- Yfreq0$row[obs]\n    j &lt;- Yfreq0$col[obs]\n    H_CCA[ind, k] &lt;- (L_CCA[i, k] + C_CCA[j, k])/sqrt(2*lambda_CCA[k]*mu_CCA[k])\n    ind &lt;- ind + 1\n  }\n}\n\n\n\nFrom canonical correlation analysis\nTo perform the cancor, we compute the inflated tables \\(R\\) (\\(\\omega \\times l\\)) and \\(C\\) (\\(\\omega \\times c\\)) from \\(Y\\) (\\(r \\times c\\)) and \\(E\\) (\\(r \\times l\\)). The difference with CA is that we use \\(E\\) instead of \\(Y\\) to compute the inflated table \\(R\\). \\(R\\) is equivalent to \\(E\\) where rows of \\(E\\) are duplicated as many times as there are correspondences in \\(Y\\).\nWe take the frequency table defined before and use it to compute the inflated tables (with weights):\n\n# Create indicator tables\ntabR &lt;- acm.disjonctif(as.data.frame(Yfreq0$row))\ntabR &lt;- as.matrix(tabR) %*% as.matrix(E) # duplicate rows of E according th the correspondences of Y\ntabC &lt;- acm.disjonctif(as.data.frame(Yfreq0$col))\ncolnames(tabC) &lt;- colnames(Y)\n\n# Get weights\nwt &lt;- Yfreq0$Freq\n\nBelow are the first lines of tables \\(R\\) and \\(C\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocation\nelevation\npatch_area\nperc_forests\nperc_grasslands\nShannonLandscapeDiv\n\n\n\n\n0\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n0\n30\n7.92\n16.4129\n43.4066\n0.274\n\n\n0\n430\n83.24\n24.4526\n28.4995\n0.274\n\n\n0\n420\n140.83\n41.9966\n34.2412\n0.260\n\n\n0\n400\n140.83\n41.9966\n34.2412\n0.260\n\n\n0\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n0\n470\n7.84\n19.3087\n56.4031\n0.253\n\n\n0\n110\n21.15\n23.4668\n49.8908\n0.262\n\n\n0\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n0\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n0\n40\n6.66\n7.4729\n66.0061\n0.240\n\n\n0\n160\n5.63\n32.3516\n45.0477\n0.253\n\n\n0\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n1\n10\n6.28\n7.7882\n67.7785\n0.232\n\n\n1\n30\n7.92\n16.4129\n43.4066\n0.274\n\n\n1\n430\n83.24\n24.4526\n28.4995\n0.274\n\n\n1\n420\n140.83\n41.9966\n34.2412\n0.260\n\n\n1\n400\n140.83\n41.9966\n34.2412\n0.260\n\n\n1\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n1\n470\n7.84\n19.3087\n56.4031\n0.253\n\n\n1\n110\n21.15\n23.4668\n49.8908\n0.262\n\n\n1\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n1\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n1\n40\n6.66\n7.4729\n66.0061\n0.240\n\n\n1\n160\n5.63\n32.3516\n45.0477\n0.253\n\n\n1\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n0\n160\n10.99\n29.7139\n18.8270\n0.291\n\n\n1\n500\n0.50\n7.5445\n67.0780\n0.240\n\n\n1\n460\n24.82\n21.3932\n57.0430\n0.243\n\n\n1\n450\n3.11\n4.8285\n73.7518\n0.216\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsp1\nsp2\nsp3\nsp4\nsp5\nsp6\nsp7\nsp8\nsp9\nsp10\nsp11\nsp12\nsp13\nsp14\nsp15\nsp16\nsp17\nsp18\nsp19\nsp21\nsp22\n\n\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nThen, we perform a canonical correlation on the scaled tables \\(R_{scaled}\\) and \\(C_{scaled}\\). We find the coefficients \\(\\rho\\) and \\(\\gamma\\) maximizing the correlation between the scores \\(S_R = R_{scaled} \\rho\\) and \\(S_C = C_{scaled} \\gamma\\).\n\n# Center scale tables\ntabR_scaled &lt;- scalewt(tabR, wt)\ntabC_scaled &lt;- scalewt(tabC, wt)\n\nres &lt;- cancor(diag(sqrt(wt)) %*% tabR_scaled, \n              diag(sqrt(wt)) %*% tabC_scaled, \n              xcenter = FALSE, ycenter = FALSE)\n# res gives the coefficients of the linear combinations that maximizes the correlation between the 2 dimensions\ndim(res$xcoef) # l columns -&gt; R_scaled is of full rank\n\n[1] 6 6\n\ndim(res$ycoef) # c-1 columns -&gt; C_scaled is not of full rank\n\n[1] 20 20\n\n# Compute these scores from this coef\nscoreR &lt;- tabR_scaled[, 1:l]  %*% res$xcoef\nscoreC &lt;- tabC_scaled[, 1:(c-1)]  %*% res$ycoef\n\nWe have \\(H = (S_R + S_C)_{scaled}\\).\n\n# Get H\nscoreRC &lt;- scoreR[, 1:l] + scoreC[, 1:l] # here l &lt; c-1 so l axes\nscoreRC_scaled &lt;- scalewt(scoreRC, wt = wt)\n\n\n\nPlot\n\n\nCode\nmultiplot(indiv_row = H_CCA, \n          indiv_row_lab = paste0(\"site \", Yfreq0$row, \"/\", Yfreq0$col),\n          row_color = \"black\", eig = lambda_CCA)"
  }
]